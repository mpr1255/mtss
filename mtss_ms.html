<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Bermond Scoggins">
<meta name="author" content="Matthew P. Robertson]">
<meta name="dcterms.date" content="2024-05-09">

<title>quarto-input6319f0fc</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="mtss_ms_files/libs/clipboard/clipboard.min.js"></script>
<script src="mtss_ms_files/libs/quarto-html/quarto.js"></script>
<script src="mtss_ms_files/libs/quarto-html/popper.min.js"></script>
<script src="mtss_ms_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="mtss_ms_files/libs/quarto-html/anchor.min.js"></script>
<link href="mtss_ms_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="mtss_ms_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="mtss_ms_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="mtss_ms_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="mtss_ms_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
</head><body class="fullcontent">\usepackage{setspace}\doublespacing
\usepackage{float}
\usepackage{setspace}
\usepackage{adjustbox}
\usepackage{xcolor}
\doublespacing
\setlength{\parindent}{0.8cm}
\setlength {\marginparwidth }{2cm}
\DeclareUnicodeCharacter{0301}{\'{e}}






<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<div style="page-break-after: always;"></div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The Royal Society has as its motto the injunction : “Take nobody’s word for it.” Yet a large portion of published studies in the social sciences demand of the reader exactly this.</p>
<p>Over the past several decades, open science advocates have called for the routinization of open science practices such as posting data and code upon a paper’s publication and the preregistration of experiments <span class="citation" data-cites="king1995replication">(<a href="#ref-king1995replication" role="doc-biblioref">King 1995</a>)</span>. Beginning principally in the psychological sciences, advocacy for these reforms rose in the 2010s due to large-scale replication failures of prominent psychological studies which highlighted the widespread presence of false positive findings <span class="citation" data-cites="Simmons2011-tp Open_Science_Collaboration2015-gk">(<a href="#ref-Simmons2011-tp" role="doc-biblioref">J. P. Simmons, Nelson, and Simonsohn 2011</a>; <a href="#ref-Open_Science_Collaboration2015-gk" role="doc-biblioref">Open Science Collaboration 2015</a>)</span>.</p>
<p>Open science practices bolster the credibility of a field and its findings by allowing readers to evaluate the methods by which researchers reach their conclusions. While trust is the currency of every epistemic community, the demand for trust alone weakens credibility. If data and code are available, interested researchers can ensure a finding’s results are computationally reproducible, robust to alternate model specifications, and error free. For experiments (i.e.&nbsp;randomised controlled trials), preregistration allows the reader to determine whether there was the selective exclusion of hypotheses, measurements, or statistical analyses that run counter to the author’s favored hypotheses.</p>
<p>Concern for research transparency has become more salient over the past decade as scholars recognize that the accumulation of false positives can drive unsuccessful decision-making and interventions. This leads to inefficient resource allocation and weakens the credibility of a field. In fields like medicine, open science practices have been strongly advocated in recognition of the direct harm that false positives can cause <span class="citation" data-cites="NAP26308 Baggerly-2009">(<a href="#ref-NAP26308" role="doc-biblioref">National Academies of Sciences and Medicine 2021</a>; <a href="#ref-Baggerly-2009" role="doc-biblioref">Baggerly and Coombes 2009</a>)</span>. Leading journals in political science and international relations are increasingly mandating the provision of data and code, as well as encouraging the preregistration of experiments.</p>
<p>We distinguish the practice of making data and code available post-publication, thereby allowing researchers to determine what we hereafter refer to as the computational reproducibility of a paper’s results, from replicibility – where new data is collected using an identical or conceptually similar design to the original paper <span class="citation" data-cites="nosek2020replication obels2020analysis">(<a href="#ref-nosek2020replication" role="doc-biblioref">Nosek and Errington 2020</a>; <a href="#ref-obels2020analysis" role="doc-biblioref">Obels et al. 2020</a>)</span>. Different fields refer to these practices in ways that can be confusing. Political science, unlike psychology, conducts fewer experimental studies and what they refer to as a replication study aims at assessing computational reproducibility <span class="citation" data-cites="king1995replication monroe2018rush">(see <a href="#ref-king1995replication" role="doc-biblioref">King 1995</a>; <a href="#ref-monroe2018rush" role="doc-biblioref">Monroe 2018</a>)</span>.</p>
<p>Political science and international relations appear to have taken open science practices seriously, with high-profile journals and academics endorsing initiatives like the Data Access and Research Transparency (DA-RT) statement. This has lead some scholars to believe that the open data problem has mostly been solved. Yet current assessments of the field’s progress have been based on relatively small samples and time-intensive human coding procedures <span class="citation" data-cites="Key2016-vr Stockemer2018-qg Grossman2020-yv">(<a href="#ref-Key2016-vr" role="doc-biblioref">Key 2016</a>; <a href="#ref-Stockemer2018-qg" role="doc-biblioref">Stockemer, Koehler, and Lentz 2018</a>; <a href="#ref-Grossman2020-yv" role="doc-biblioref">Grossman and Pedahzur 2020</a>)</span>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Our paper presents the largest-scale study of open science practices in political science and international relations thus far; it is also the first systematic study of the prevalence of preregistration in experiments in these fields. Our study spans the years 2010 to 2021 and includes population-level data, allowing us illustrate trends in specific journals. Documenting such trends is important given the key role played by journals in promulgating and enforcing transparent research norms.</p>
<p>We ask two questions: (1) What proportion of papers that rely on statistical inference make their data and code public? (2) What proportion of experimental studies were preregistered? We gather 93,931 published articles from the top 160 journals ranked by Clarivate’s Journal Citation Reports <span class="citation" data-cites="JCR2020">(<a href="#ref-JCR2020" role="doc-biblioref">2020</a>)</span> and use machine learning classifiers to identify either statistical inference or experimental papers.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> We identify which had open data and preregistration using public application programming interfaces (API), text analysis, and web scraping.</p>
<section id="the-state-of-open-political-science-practices" class="level2">
<h2 class="anchored" data-anchor-id="the-state-of-open-political-science-practices">The state of open political science practices</h2>
<p>Since the onset of the replication crisis, how much of the literature dependent on data and statistical inference still relies solely on reader trust? Extant research on the prevalence of open data practices in political science paints a sobering picture. Stockemer, Koehler, and Lentz’s <span class="citation" data-cites="Stockemer2018-qg">(<a href="#ref-Stockemer2018-qg" role="doc-biblioref">2018</a>)</span> analysis of 145 quantitative studies published in three journals during 2015 found that only 55% provided original data and 56% provided code.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> An earlier analysis, conducted on 494 quantitative articles in six leading political science journals between 2013 and 2014, found that full computational reproducibility materials (data and code) were available for only 58% of papers <span class="citation" data-cites="Key2016-vr">(<a href="#ref-Key2016-vr" role="doc-biblioref">Key 2016</a>)</span>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Poor data availability affects many natural and social science disciplines <span class="citation" data-cites="Culina2020-xr Errington2021-zg">(<a href="#ref-Culina2020-xr" role="doc-biblioref">Culina et al. 2020</a>; <a href="#ref-Errington2021-zg" role="doc-biblioref">Errington et al. 2021</a>)</span>. A random sample of 250 psychology papers published between 2014 and 2017 estimated that 14% of papers shared research materials, 2% provided original data, and 1% shared their code <span class="citation" data-cites="Hardwicke2021-fp">(<a href="#ref-Hardwicke2021-fp" role="doc-biblioref">Hardwicke, Thibault, et al. 2021</a>)</span>. Preregistration was rare (3%). Similarly, even once data is shared, analytic reprodubility is not guaranteed <span class="citation" data-cites="hardwicke2021analytic">(<a href="#ref-hardwicke2021analytic" role="doc-biblioref">Hardwicke, Bohn, et al. 2021</a>)</span>.</p>
<p>A tonic for many of these problems is straightforward: computational reproducibility materials for all quantitative studies and preregistration for experiments. Reproducibility materials and preregistration militates against questionable research practices (QRPs) that lead to false positives by constraining researcher degrees of freedom and ensuring that key decisions made in the analysis process are transparent to peers.</p>
<p>In the behavioural sciences, false positives can arise from decisions that are rationalised as legitimate by authors: failing to report all dependent variables in a study, collecting more data after seeing whether the results were statistically significant, failing to report all conditions, stopping data collection after achieving the desired result, rounding down p-values, selectively reporting studies that ‘worked’, selectively excluding observations, and claiming an unexpected finding was predicted (or hypothesising after results are known). However, these practices obfuscate the uncertainty around a particular set of claims and mislead readers into being overconfident about a study’s conclusions.</p>
<p>The use of QRPs appears to be widespread in many of the social sciences. Surveys of psychology and criminology researchers report they routinely do not report all dependent variables, collect more data after peeking at results, and selectively report statistically significant studies <span class="citation" data-cites="John2012-vj Chin2021-px">(<a href="#ref-John2012-vj" role="doc-biblioref">John, Loewenstein, and Prelec 2012</a>; <a href="#ref-Chin2021-px" role="doc-biblioref">Chin et al. 2021</a>)</span>. Other methods of detecting publication bias, such as analysing sets of studies or literatures using a p-curve or z-curve, reveal extensive clustering of p-values (z-scores) just past p &lt; 0.05 <span class="citation" data-cites="Simonsohn2014-qg Bartos_2020-ar">(<a href="#ref-Simonsohn2014-qg" role="doc-biblioref">Simonsohn, Nelson, and Simmons 2014</a>; <a href="#ref-Bartos_2020-ar" role="doc-biblioref">Bartoš and Schimmack 2020</a>)</span>. Examples of these problems in the behavioural and social sciences range from the power posing literature <span class="citation" data-cites="Simmons2017-rs">(<a href="#ref-Simmons2017-rs" role="doc-biblioref">J. P. Simmons and Simonsohn 2017</a>)</span> to economic research using instrumental variables and difference-in-differences <span class="citation" data-cites="Brodeur2020-ld">(<a href="#ref-Brodeur2020-ld" role="doc-biblioref">Brodeur, Cook, and Heyes 2020</a>)</span>.</p>
<p>In recognition of these problems, professional organisations in political science and international relations, including the American Political Science Association (APSA), have led efforts to increase the availability of data and code that accompany published papers. The DA-RT statement developed by the APSA council in 2014 involved a commitment by journal editor signatories to increase the availability of data “at the time of publication through a trusted digital repository”, as well as require authors to “delineate clearly the analytic procedures upon which their published claims rely, and where possible to provide access to all relevant analytic materials” <span class="citation" data-cites="JETS_2015-mm">(<a href="#ref-JETS_2015-mm" role="doc-biblioref">Statement 2015</a>)</span>.</p>
<p>While there was an intramural debate about how DA-RT standards would affect qualitative work, given the heterogeneity of interview data and other forms of qualitative analysis, we bypass these arguments in this paper by focusing exclusively on papers relying on statistical inference.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> It is relatively straightforward for researchers using statistical inference to release the very data and code that were necessary to produce the results in their papers. As Key <span class="citation" data-cites="Key2016-vr">(<a href="#ref-Key2016-vr" role="doc-biblioref">2016</a>)</span> notes, the internet has reduced the cost for journals to set up Dataverse repositories and made it easier for researchers to share their data and code. Rising usage of free statistical programming software, such as <code>R</code> and its desktop application RStudio, also reduces barriers to computational reproducibility.</p>
<p>The 27 journal editors who adopted the statement agreed to implement reforms by January 2016. Of the 16 DA-RT signatory journals in our dataset, two made no change in practice and a further four have data and code that is difficult to accurately estimate.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
</section>
<section id="the-need-for-open-data" class="level2">
<h2 class="anchored" data-anchor-id="the-need-for-open-data">The need for open data</h2>
<section id="uncovering-data-errors-and-misinterpretation" class="level3">
<h3 class="anchored" data-anchor-id="uncovering-data-errors-and-misinterpretation">Uncovering data errors and misinterpretation</h3>
<p>Errors in data or the misreporting of p-values or test statistics invariably occur in research and can go undetected by an article’s authors or peer reviewers. These problems, if addressed, may substantively alter an article’s conclusions or produce null rather than positive results. Reporting errors in regression coefficients or test statistics occur frequently <span class="citation" data-cites="Nuijten2016-dp">(<a href="#ref-Nuijten2016-dp" role="doc-biblioref">Nuijten et al. 2016</a>)</span>.</p>
<p>Access to the original data can help determine whether errors are trivial, and contribute to retraction efforts if they are not <span class="citation" data-cites="YouthSoc_2020-ut">(<a href="#ref-YouthSoc_2020-ut" role="doc-biblioref"><span>“Retraction Notice”</span> 2020</a>)</span>. In some cases, access to the data allows for detailed concerns with a paper’s analysis to be illustrated without the journal believing a retraction is warranted <span class="citation" data-cites="Hilgard_2020-lg Hilgard_2021_wb">(see <a href="#ref-Hilgard_2020-lg" role="doc-biblioref">Hilgard 2020</a>, <a href="#ref-Hilgard_2021_wb" role="doc-biblioref">2021</a>)</span>.</p>
</section>
<section id="identifying-model-misspecification-and-facilitating-extension" class="level3">
<h3 class="anchored" data-anchor-id="identifying-model-misspecification-and-facilitating-extension">Identifying model misspecification and facilitating extension</h3>
<p>Researchers have tremendous flexibility in deciding how to collect data and which statistical models should be specified to analyse them. Andrew Gelman has termed this process the ‘garden of the forking paths’ <span class="citation" data-cites="Gelman2014-wz">(<a href="#ref-Gelman2014-wz" role="doc-biblioref">2014</a>)</span>: some set of decisions might yield a positive result, while another set of equally justifiable decisions might lead to a null result. The replication crisis has shown that it is a mistake to view a single study or set of statistical analyses as a definitive answer to a given theory or claim — the scientific process should instead be iterative, exploratory, and cumulative <span class="citation" data-cites="Tong2019-as">(<a href="#ref-Tong2019-as" role="doc-biblioref">Tong 2019</a>)</span>.</p>
<p>Open data can address the problem of model misspecification and uncertainty around modelling the data generating process <span class="citation" data-cites="Neumayer2017-hc">(<a href="#ref-Neumayer2017-hc" role="doc-biblioref">Neumayer and Plümper 2017</a>)</span>. Since researchers cannot anticipate changes to methodological best practices, computational reproducibility materials allow scholars to make adjustments if best practices change.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> Even if misspecification is not a problem, extending and building off of the original analyses – to run more theoretically motivated models, sensitivity analyses, or assess treatment heterogeneity – are net positives for science <span class="citation" data-cites="Janz2016-bt">(<a href="#ref-Janz2016-bt" role="doc-biblioref">Janz 2016</a>)</span>.</p>
</section>
<section id="exposing-data-falsification" class="level3">
<h3 class="anchored" data-anchor-id="exposing-data-falsification">Exposing data falsification</h3>
<p>In the most egregious cases, open data allows researchers to investigate and expose data falsification. High-profile exposures of data falsification include the LaCour and Green <span class="citation" data-cites="LaCour2014-ul">(<a href="#ref-LaCour2014-ul" role="doc-biblioref">2014</a>)</span> case in political science, and the Shu et al. <span class="citation" data-cites="Shu2012-nr">(<a href="#ref-Shu2012-nr" role="doc-biblioref">2012</a>)</span> case in psychology <span class="citation" data-cites="Kristal2020-rh Leif2021-fo">(see <a href="#ref-Kristal2020-rh" role="doc-biblioref">Kristal et al. 2020</a>; <a href="#ref-Leif2021-fo" role="doc-biblioref">Nelson, Simonsohn, and Simmons 2021</a>)</span>. Both rested on investigator access to the original data. While presumably data falsification is exceedingly rare, there is no way to know its extent given the general absence of computational reproducibility materials in the first place.</p>
</section>
</section>
<section id="the-need-for-preregistration" class="level2">
<h2 class="anchored" data-anchor-id="the-need-for-preregistration">The need for preregistration</h2>
<!-- Simmons, Nelson, and Simonsohn -- motivated reasoning   

Nosek - hindsight bias

Lakens -- prereg as severity testing

Scheel et al -- discussion of how registered reports have fewer positive results than non-published results
-->
<section id="distinguishing-confirmatory-from-exploratory-analysis" class="level3">
<h3 class="anchored" data-anchor-id="distinguishing-confirmatory-from-exploratory-analysis">Distinguishing confirmatory from exploratory analysis</h3>
<p>Preregistration means that researchers specify their hypotheses, measurements, and analytic plans prior to running an experiment. This commits researchers to making theoretical predictions before they can view the data and be influenced by observing the outcomes <span class="citation" data-cites="Simmons2011-tp Simmons2021-qm">(<a href="#ref-Simmons2011-tp" role="doc-biblioref">J. P. Simmons, Nelson, and Simonsohn 2011</a>; <a href="#ref-Simmons2021-qm" role="doc-biblioref">J. Simmons, Nelson, and Simonsohn 2021</a>)</span>. By temporally separating predictions from the data that tests their accuracy, there is much less flexibility for both post hoc theorising and alterations of statistical tests to fit the prediction.</p>
<p>Post hoc theorising, also known as hypothesising after the results are known (HARKing), is an example of circular logic — the researcher conducts many tests when exploring a dataset, the data reveals a relationship that can be made into a hypothesis, and that hypothesis is ‘tested’ on the data that generated it <span class="citation" data-cites="Nosek2018-an">(<a href="#ref-Nosek2018-an" role="doc-biblioref">Nosek et al. 2018</a>)</span>. But the diagnosticity of a p-value is in part predicated on knowing how many tests were performed: when an exploratory finding is reported as a prediction, the normal methods employed to evaluate the validity of a hypothesis — such as whether the p-value is less than 0.05 (i.e.&nbsp;null hypothesis significance testing) — no longer hold. P-values in that case have unknown diagnositicity <span class="citation" data-cites="Nosek2018-an">(<a href="#ref-Nosek2018-an" role="doc-biblioref">Nosek et al. 2018</a>)</span>. Thus, post hoc theorising and selective reporting greatly contribute to false positives.</p>
</section>
<section id="reducing-the-selective-reporting-of-results" class="level3">
<h3 class="anchored" data-anchor-id="reducing-the-selective-reporting-of-results">Reducing the selective reporting of results</h3>
<p>The selective reporting of statistical tests and results can occur for a variety of reasons. There are numerous legitimate ways of analyzing data, and this makes selective reporting seem justifiable. Danger arises when researchers convince themselves that the measures and tests lending evidence to their claims are the ‘right’ ones, while unjustifiably failing to report measures and tests that did not support the favored hypothesis.</p>
<p>Selectively reported experimental studies often result in overconfident theoretical claims and inflated effect sizes when compared to replications. The Open Science Collaboration <span class="citation" data-cites="Open_Science_Collaboration2015-gk">(<a href="#ref-Open_Science_Collaboration2015-gk" role="doc-biblioref">2015</a>)</span> and Many Labs studies <span class="citation" data-cites="Klein2014-sm Klein2018-gr">(<a href="#ref-Klein2014-sm" role="doc-biblioref">2014</a>, <a href="#ref-Klein2018-gr" role="doc-biblioref">2018</a>)</span> have shown that the effect sizes in highly powered replications are much smaller than those in the original studies. When reforms are implemented mandating preregistration, by research bodies or formats like registered reports, the number of null results reported rise <span class="citation" data-cites="Kaplan2015-fj Scheel2021-wa">(<a href="#ref-Kaplan2015-fj" role="doc-biblioref">Kaplan and Irvin 2015</a>; <a href="#ref-Scheel2021-wa" role="doc-biblioref">Scheel, Schijen, and Lakens 2021</a>)</span>.</p>
<p>The primary purpose of preregistration is to provide journal reviewers and readers the ability to transparently evaluate predictions and the degree of flexibility researchers had to arrive at their conclusions <span class="citation" data-cites="Lakens2019-lt Claesen2019-th Franco2014-nw">(<a href="#ref-Lakens2019-lt" role="doc-biblioref">Lakens 2019</a>; <a href="#ref-Claesen2019-th" role="doc-biblioref">Claesen et al. 2019</a>; <a href="#ref-Franco2014-nw" role="doc-biblioref">Franco, Malhotra, and Simonovits 2014</a>)</span>. It is up to the reader to determine whether preregistered studies followed their preregistration plan and adequately justified deviations – insufficiently detailed preregistration reports are an ongoing problem <span class="citation" data-cites="Ofosu_2020-xo">(<a href="#ref-Ofosu_2020-xo" role="doc-biblioref">Ofosu and Posner 2020</a>)</span>.</p>
<p>The replication crisis has altered best practices and changed the habits of many researchers in the behavioural sciences. As we show below, preregistration is not yet the norm in political science and international relations. The conclusions from many studies relying on statistical inference, even some that that have been preregistered on a registry, remain exposed to the statistical pitfalls described above.</p>
</section>
</section>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<p>Our study design called for a comprehensive analysis of population-level data, yet our populations — (1) papers using data and statistics, and (2) original experiments — were embedded in a larger population of <em>all</em> political science and international relations publications in target journals. We downloaded all of the journals’ papers from 2010 to 2021. Once we had these papers locally, we identified the data, statistical, and experimental papers through dictionary-based feature engineering and machine learning. We then used public APIs, web scraping, and text analysis to identify which of the studies had computational reproducibility materials. We outline this process below.</p>
<section id="phase-one-gathering-and-classifying-the-papers" class="level2">
<h2 class="anchored" data-anchor-id="phase-one-gathering-and-classifying-the-papers">Phase one: gathering and classifying the papers</h2>
<p>We used Clarivate’s 2021 Journal Citation Report to identify target journals. We filtered for the top 100 journals in both political science and international relations, and combined the two lists for a total of 176 journals.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<p>With this list, we used the Crossref API to download all publication metadata. We were able to obtain records for 162 journals. This resulted in over 445,000 papers, which we then filtered on Crossref’s <code>published.print</code> field for 2010 and onwards, resulting in 109,553 papers. We used the <code>published.print</code> field as it was the only reliable indicator of actual publication date, and the most complete.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> As of mid-2023 we were able to obtain 93,931 of these articles in PDF and HTML formats, and we use this as the denominator in the study. We converted the PDFs to plaintext using the open source command line utility <code>pdftotext</code>, and we converted the HTML files to text using the <code>html2text</code> utility.</p>
<p>Identifying the papers that relied on data, statistical analysis, and experiments was an iterative process. In each case we read target papers and devised a dictionary of terms meant to uniquely identify others like them. We extensively revised these dictionaries to arrive at terms that seemed to maximally discriminate for target reports. The dictionaries eventually comprised 52, 180, and 133 strings, symbols, or regular expressions for the three categories respectively.</p>
<p>The dictionaries were then used with custom functions to create document feature matrices (DFM), where each paper is an observation, each column a dictionary term, and each cell a count of that term.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> The DFM format made the papers amenable to large-scale analysis. In machine learning parlance, this process is known as feature engineering.</p>
<p>For the first research question – examining the presence of code and data in papers involving statistical inference – we hand-coded a total of 1,624 papers with boolean categories and identified 585 that relied on statistical inference. We defined statistical inference papers as any that involved mathematical modeling of data. This definition is meant to capture a simple idea: mathematical modeling requires computer instructions that perform functions on numbers. In the absence of computational reproducibility materials, these transformations cannot be exactly reproduced by readers. We also developed a dictionary of 35 terms for formal theory papers, because we wished to exclude papers that did not apply a model to real-world data.</p>
<p>For the second question — examining what proportion of experiments were preregistered — we hand-coded 518 papers with a single boolean category: whether the paper reported one or more original experiments. We defined this as any article containing an experiment where the researchers had control over treatment.</p>
<p>We then trained two machine learning models — the Support Vector Machine (SVM) and Naive Bayes (NB) binary classifiers — to arrive at estimates for the total number of statistical inference and experimental papers.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> SVMs are a pattern recognition algorithms that give binary classifications to variables in high-dimensional feature space by finding the optimal separating boundary between labeled training data <span class="citation" data-cites="James2021-mw Cristianini2000-bu">(<a href="#ref-James2021-mw" role="doc-biblioref">James et al. 2021, 337–72</a>; <a href="#ref-Cristianini2000-bu" role="doc-biblioref">Cristianini and Shawe-Taylor 2000</a>)</span>. The NB family of algorithms calculate the posterior probability of a given classified input based on the independent probability of all the values of its features; it then applies this trained algorithm to classify new inputs <span class="citation" data-cites="Rhys2020-uc">(<a href="#ref-Rhys2020-uc" role="doc-biblioref">Rhys 2020, 135–67</a>)</span>.</p>
<p>We report the SVM model results both for their greater accuracy and due to our theoretical prior that the model would be more suitable for a high-dimensional classification problem. For the first research question, our SVM model achieved 92.35% accuracy for statistical papers. For the classifying experiments, the accuracy was 86.05%. In Appendix 1 we report the confusion matrices, hyperparemeter tuning data, and NB models.</p>
<p>The application of the SVM model to the full dataset of 93,931 publications leads to an estimate of 24,026 using statistical inference.</p>
<p>The identification of experimental papers proceeded slightly differently. Rather than beginning with the full corpus, we first filtered for only the papers that included the word “experiment” over five times (4,835). We then ran the SVM classifier on this subset. The resulting estimate was 2,552 papers reporting experiments.</p>
</section>
<section id="phase-two-identifying-open-data-and-preregistrations" class="level2">
<h2 class="anchored" data-anchor-id="phase-two-identifying-open-data-and-preregistrations">Phase two: Identifying open data and preregistrations</h2>
<p>We attempted to identify open data resources in seven ways.</p>
<ol type="1">
<li>Using the Harvard Dataverse API, we downloaded all datasets held by all journals in our corpus who maintained their own, named dataverse (n=20);</li>
<li>We queried the Dataverse for the titles of each of the 109,553 papers in our corpus and linked them to their most likely match with the aid of a custom fuzzy string matching algorithm. We validated these matches and manually established a string-similarity cut-off, setting aside the remainder;</li>
<li>We extracted from the full text of each paper in our corpus the link to its dataset on the Dataverse (1,142; note this had significant overlap with the results of the first and second queries);</li>
<li>We downloaded the metadata listing the contents of these datasets, to confirm firstly that they had data in them, and secondly that it did not consist of only pdf or doc files. In cases where a list of metadata was not available via the Dataverse API, we scraped the html of the dataset entry and searched for text confirming the presence of data files;</li>
<li>We used regular expressions to extract from the full text papers references to “replication data,” “replication materials,” “supplementary files” and similar terms, then searched in the surrounding text for any corresponding URLs or mentions of author personal websites or other repositories<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>. We validated these results by exporting various combination of string matches with the above terms to Excel files, where we examined them in tabular format and validated their relevance. Given that replication and supplementary material stored on personal websites is not of the same order as material on the Dataverse and similar repositories, these results are recorded in our results under the rubric of ‘precarious data’;</li>
<li>We searched all of the full text papers for references to other repositories, including Figshare, Dryad, and Code Ocean, using regular expressions. Where found, these were recorded as containing replication data, the same as the Dataverse;</li>
<li>As additional validation for DA-RT signatory journals specifically, we downloaded the html file corresponding to each article and/or the html file hosting supplemental material (n=2,284), then extracted all code and data-related file extensions to establish their open data status.</li>
</ol>
<p>We attempted to identify preregistration of experiments in the following ways:</p>
<ol type="1">
<li>We used regular expressions to extract from all of the experimental papers sentences that referred to “prereg” or “pre-reg”, “preanalysis” or “pre-analysis”, as well as any references to commonly used preregistration servers (OSF, EGAP, and AsPredicted), and then searched for the availability of the corresponding link to validate that the preregistration had taken place. Parts of this process — for instance, searching author names in the Experiments in Governance and Politics (EGAP) registry to look for the corresponding paper — involved time-consuming detective work;</li>
<li>We downloaded all EGAP preregistration metadata in JSON format from the Open Science Foundation Registry (https://osf.io/registries/discover), extracted from this file all osf.io links and unique EGAP registry IDs, and used command line utilities to search for them through the corpus of all the papers.</li>
</ol>
<p>We did not examine whether the published report conformed to the preregistration plan.</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<!-- Graphs with bars over time for total stat papers in dataset

Figure 1 illustrates the annual trend in the percentage of political science and international relations papers with some form of open data (left hand y axis), as well as display the total number of statistical inference papers detected in every year (right hand y axis). 
-->
<p>Statistical inference papers are infrequently accompanied by the datasets or code that generated their findings. For the 12 year period under observation, we were able to match 21% of statistical inference articles to data repositories (overwhelmingly the Harvard Dataverse). Encouragingly, Figure 1 shows that the percentage of open data has increased between 2010 and 2021 – rising steadily from about 11% to 26% during this period.</p>
<!-- 
![Open data in statistical inference papers by journal (with over 200 papers)](./graphs/od_journal.png){width=90%}
-->
<p>The total number of statistical inference papers have gradually increased during the 12 year period. In 2010, we found 1,329 papers and 2,640 in 2020 – the last year with complete data. This supports King’s <span class="citation" data-cites="King1990-hy">(<a href="#ref-King1990-hy" role="doc-biblioref">1990</a>)</span> observation that political science and international relations have long been disciplines increasingly concerned with quantitative methods.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> While the percentage of papers with open data have increased, so too have the absolute number of statistical papers without it. There are simply more published papers making inferences based on hidden data.</p>
<!-- 
![Open data in statistical inference papers by year published in 16 journals signatory to the DA-RT statement](./graphs/dart_grid.png){width=85%}
-->
<p>There are significant differences in open data practices between journals. Figure 2 displays the percentage of statistical inference papers with open data in the 41 journals with over 200 such papers.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> The number above each journal’s bin represents the number of statistical inference papers detected by the support vector machine classifier. Of the 41 journals, 11 have over 50% open data, and 16 have over 20%.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
<p>The effectiveness of the DA-RT statement on journal open data practices is illustrated in Figure 3, which displays the percentage of statistical inference papers with open data by year in each of the 16 DA-RT signatory journals we consider.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></p>
<p>Four journals – , , , and – already made significant progress prior to the release of the initial DA-RT guidelines in 2014. Many of the remaining journals either made significant progress in 2016 or shortly thereafter.</p>
<p>One caveat is that is that 2 of the 16 journal signatories have consistently low levels of open data even after DA-RT reforms were agreed to commence on January 15, 2016. The extent of transparent practices in three other journals – , , and – was more difficult to determine, given they did not use the Harvard Dataverse. Our attempt to estimate data and code availability for such journals, noted in point seven of phase two of the methods section, appears to produce unreliable and puzzling results.</p>
<!-- Quarterly Journal of Political Science cannot be accessed through ANU. Is this the case for International Security, Cooperation and Conflict, and Security Studies? -->
<!-- 
![Preregistration in experiments by year](./graphs/prereg_time.png){width=80%}
-->
<p>The preregistration of experiments is rare in political science and international relations journals. Figure 4 shows that the first preregistered study in the dataset that we could identify was in 2013, and that the rate of preregistration only began climbing in 2016. The proportion of experiments that were preregistered for the entire period is approximately 5%; the annual rate has slowly risen to 16% in 2021.</p>
<!-- 
![Preregistration in experiments by journal (with over 20 papers)](./graphs/prereg_journal.png)
-->
<p>Figure 5 shows the percentage of experiments that were preregistered in the 29 journals with more than 20 experiments. Only the exceeds 20%. Unlike with open data, when it comes to preregistration the differences between journals are small. Of the experiments published in and , the two journals with the most experiments that bridge the gap between political science and psychology, only four and five percent respectively are preregistered.</p>
<p>Prior to the replication crisis at the beginning of the 2010s, there were no organized attempts at enforcing preregistration or using registered reports as a way of curbing researcher flexibility and its attendant QRPs. As psychology was among the first of the sciences to reckon with its methodological issues, brought to light in part by such articles as Simmons, Nelson, and Simonsohn’s <span class="citation" data-cites="Simmons2011-tp">(<a href="#ref-Simmons2011-tp" role="doc-biblioref">2011</a>)</span>, it is logical that it took several years for these new practices to be adopted in contiguous disciplines like political science and international relations. But our data illustrate that significant improvements must be made in order for experiments in these fields to meet current methodological best practices.</p>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Scientists must carry out their work while simultaneously signalling and vouchsafing for its credibility. For the pioneers of the scientific method in 17th century Europe, this included an ensemble of rhetorical and social practices, including the enlistment of trusted witnesses to testify that experiments had in fact taken place as claimed – this is what Shapin refers to as the moral economy of science <span class="citation" data-cites="Shapin2018-ir Shapin1995-lj">(<a href="#ref-Shapin2018-ir" role="doc-biblioref">Shapin 2018, 84, 107–8</a>; <a href="#ref-Shapin1995-lj" role="doc-biblioref">1995</a>)</span>.</p>
<p>In the digital age, we argue that the credibility of social science must largely rest on computational reproducibility. <!-- This sentence needs to be edited:  Given that the burden of enabling reproducibility for any given social scientist is low -- the provision of code and data needed to produce the study in the first place is often enough; few provide instructions for the reproduction of an entire computational environment [@Roukema2021-xq] -- there is a limited number of legitimate reasons for withholding computational reproducibility materials. --> The same goes for preregistration and experiments. Adhering to these practices ensures other social scientists can check and reproduce the findings, that the findings are valid, and also demonstrates a commitment to the norm of science as a shared enterprise.</p>
<p>The chief reason for depositing code and data is not for signalling: Open science practices provide the reader with an opportunity to transparently evaluate the evidence for a set of claims and scrutinise an article for any of the myriad problems that plague the use of data and statistical models. An interested reader could investigate an article’s data and code for errors, determine whether results are robust to different model specifications, or, in rare cases, detect data falsification. <!-- Needs clarification: It is likely that universal open data would significantly deter any potential falsification in the first place. --> For experiments, the published paper can be compared to the preregistration document to determine whether there were any unjustified deviations.</p>
<p>Our findings show that political science and international relations are not currently living up to these best practices. For the approximately 25,000 statistical inference papers in the dataset, we could only identify approximately 21% that had a corresponding data respository. Despite improvement in most years, change has not been uniform across the discipline — most of the progress has been made by a handful of the highest impact factor journals. In 2020, for example, 16 out of the 52 journals with over 20 statistical inference papers had an open data percentage over 50% (see Figure A6) – 20 journals had an open data percentage over 20%. We could not locate data or code for two of the 16 DA-RT signatories in our dataset.</p>
<p>Universal open data is a collective action problem, and it is the responsibility of journals to foster and enforce these disciplinary norms. In the absence of that, individual researchers do not always share data, and requesting it can sometimes be mistaken as a gesture of challenge rather than collegiality. As Simonsohn <span class="citation" data-cites="Simonsohn2013-fa">(<a href="#ref-Simonsohn2013-fa" role="doc-biblioref">2013</a>)</span> notes, the modal response to his requests for original data was that the data was no longer available. We suspect that variation in open data practices between journals reflects differences in journal editors’ views of its importance for research credibility.</p>
<!-- Edit this in light of JPR and JCR: An outlier in the dataset is the international relations journal \emph{International Interactions} (II), which has an average open data rate of approximately 90%. It is the only journal in the dataset that consistently published the majority of its statistical inference articles with open data in the early 2010s. II's policies are straightforward: submissions dependent on datasets must provide a DOI to the computational reproducibility materials on the Harvard Dataverse, and the submission must be computationally reproducible by II staff [@intinterac2021]. II and the other journals show that this policy can be successfully enforced. Omitting the requirement for computational reprodubility would reduce the burden on journal editors further. -->
<p>The DA-RT initiative sparked spirited debate in the field about the provision of data and code — but the same cannot be said for preregistration. Experiments are rarely preregistered. Of the roughly 2,552 experiments in our dataset, 5% are preregistered. Given that the use of experiments only began to take off in 2014, as shown in Figure 3, the proportion of preregistered experiments in the literature is understandably low. Fortunately, the trend is positive. Two journals of 26 with more than 5 published experiments had a preregistration percentage of over 30% in 2020 (see Figure A7).</p>
<p>Identifying whether an experiment had a corresponding preregistration report was at times difficult. Numerous experiments made no mention of their preregistration report in the manuscript despite having one listed in a repository. Locating it was also difficult given changing manuscript titles and authors. Their omission in the manuscript is likely due to the fact that many journal editors do not determine whether an experiment has a preregistration or pre-analysis plan or request their disclosure.<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
<p>The difficulty of matching an experiment with its preregistration report is far smaller than matching a manuscript to a concealed preregistration report. A unique and unanticipated problem we found were authors publishing a study where they omitted any reference to a preregistered experiment – ostensibly due to null findings. <span class="citation" data-cites="byun2021geopolitical">Byun, Kim, and Li (<a href="#ref-byun2021geopolitical" role="doc-biblioref">2021</a>)</span> use their survey data to make descriptive claims while failing to discuss the design or results of their experimental manipulation <span class="citation" data-cites="Kim2021-prereg">(<a href="#ref-Kim2021-prereg" role="doc-biblioref">Kim, Byun, and Li 2021</a>)</span>. It is not clear whether their results failed to further their own argument or were possibly disconfirmatory. In either situation, readers are not permitted to transparently evaluate the strength of their claims.</p>
<p>Peer reviewers and readers of published works routinely examine whether a theory or explanation has appropriate evidence; whether the measurements are valid and reliable; whether the model has been appropriately specified. Here, we prompt referees and readers to also begin asking: (1) Are the computational reproducibility materials on the Harvard Dataverse or some other reliable repository? (2) Is the paper computationally reproducible based on those materials? (3) If an experiment, was it preregistered? (4) Does the analysis in the experimental paper follow the preregistration plan and are deviations from that plan justified?<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> We hope that evaluating scientific research in this manner will help move readers away from trusting research in the absence of open science practices to a more informed trust in their presence.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>Both authors acknowledge support from the Australian Government Research Training Program Scholarship, the ANU Library, Taylor &amp; Francis, TeamViewer AG, and the Google Cloud Research Credits program (award GCP19980904).</p>
<div style="page-break-after: always;"></div>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Baggerly-2009" class="csl-entry" role="listitem">
Baggerly, Keith A., and Kevin R. Coombes. 2009. <span>“<span class="nocase">Deriving chemosensitivity from cell lines: Forensic bioinformatics and reproducible research in high-throughput biology</span>.”</span> <em>The Annals of Applied Statistics</em> 3 (4): 1309–34. <a href="https://doi.org/10.1214/09-AOAS291">https://doi.org/10.1214/09-AOAS291</a>.
</div>
<div id="ref-Bartos_2020-ar" class="csl-entry" role="listitem">
Bartoš, František, and Ulrich Schimmack. 2020. <span>“Z-Curve 2.0: Estimating Replication Rates and Discovery Rates.”</span> <a href="https://psyarxiv.com/urgtn/download?format=pdf" class="uri">https://psyarxiv.com/urgtn/download?format=pdf</a>.
</div>
<div id="ref-Brodeur2020-ld" class="csl-entry" role="listitem">
Brodeur, Abel, Nikolai Cook, and Anthony Heyes. 2020. <span>“Methods Matter: P-Hacking and Publication Bias in Causal Analysis in Economics.”</span> <em>Am. Econ. Rev.</em> 110 (11): 3634–60.
</div>
<div id="ref-byun2021geopolitical" class="csl-entry" role="listitem">
Byun, Joshua, DG Kim, and Sichen Li. 2021. <span>“The Geopolitical Consequences of COVID-19: Assessing Hawkish Mass Opinion in China.”</span> <em>Political Science Quarterly</em> 136 (4): 641–65.
</div>
<div id="ref-Chin2021-px" class="csl-entry" role="listitem">
Chin, Jason M, Justin T Pickett, Simine Vazire, and Alex O Holcombe. 2021. <span>“Questionable Research Practices and Open Science in Quantitative Criminology.”</span> <em>J. Quant. Criminol.</em>, August.
</div>
<div id="ref-Claesen2019-th" class="csl-entry" role="listitem">
Claesen, Aline, Sara L B T Gomes, Francis Tuerlinckx, and Wolf Vanpaemel. 2019. <span>“Preregistration: Comparing Dream to Reality.”</span>
</div>
<div id="ref-JCR2020" class="csl-entry" role="listitem">
Clarivate. 2020. <span>“Journal Citation Reports.”</span> 2020.
</div>
<div id="ref-Cristianini2000-bu" class="csl-entry" role="listitem">
Cristianini, Nello, and John Shawe-Taylor. 2000. <em>An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods</em>. Cambridge University Press.
</div>
<div id="ref-Culina2020-xr" class="csl-entry" role="listitem">
Culina, Antica, Ilona van den Berg, Simon Evans, and Alfredo Sánchez-Tójar. 2020. <span>“Low Availability of Code in Ecology: A Call for Urgent Action.”</span> <em>PLoS Biol.</em> 18 (7): e3000763.
</div>
<div id="ref-Errington2021-zg" class="csl-entry" role="listitem">
Errington, Timothy M, Alexandria Denis, Nicole Perfito, Elizabeth Iorns, and Brian A Nosek. 2021. <span>“Challenges for Assessing Replicability in Preclinical Cancer Biology.”</span> <em>Elife</em> 10 (December).
</div>
<div id="ref-Franco2014-nw" class="csl-entry" role="listitem">
Franco, Annie, Neil Malhotra, and Gabor Simonovits. 2014. <span>“Unlocking the File Drawer.”</span> <em>Science</em> 345 (6203): 1502–5.
</div>
<div id="ref-Gelman2014-wz" class="csl-entry" role="listitem">
Gelman, Andrew, and Eric Loken. 2014. <span>“The Statistical Crisis in Science: Data-Dependent Analysis–a <span>‘Garden of Forking Paths’</span>–Explains Why Many Statistically Significant Comparisons Don’t Hold Up.”</span> <em>Am. Sci.</em> 102: 460+.
</div>
<div id="ref-Grossman2020-yv" class="csl-entry" role="listitem">
Grossman, Jonathan, and Ami Pedahzur. 2020. <span>“Can We Do Better? Replication and Online Appendices in Political Science.”</span> <em>Perspectives on Politics</em>, 1–6.
</div>
<div id="ref-hardwicke2021analytic" class="csl-entry" role="listitem">
Hardwicke, Tom E, Manuel Bohn, Kyle MacDonald, Emily Hembacher, Michèle B Nuijten, Benjamin N Peloquin, Benjamin E DeMayo, Bria Long, Erica J Yoon, and Michael C Frank. 2021. <span>“Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal Psychological Science: An Observational Study.”</span> <em>Royal Society Open Science</em> 8 (1): 201494.
</div>
<div id="ref-Hardwicke2021-fp" class="csl-entry" role="listitem">
Hardwicke, Tom E, Robert T Thibault, Jessica E Kosie, Joshua D Wallach, Mallory C Kidwell, and John P A Ioannidis. 2021. <span>“Estimating the Prevalence of Transparency and Reproducibility-Related Research Practices in Psychology (2014-2017).”</span> <em>Perspect. Psychol. Sci.</em>, March.
</div>
<div id="ref-Hilgard_2020-lg" class="csl-entry" role="listitem">
Hilgard, Joseph. 2020. <span>“Curious Features of Data in Zhang Et Al. (2019).”</span> OSF.
</div>
<div id="ref-Hilgard_2021_wb" class="csl-entry" role="listitem">
———. 2021. <span>“I Tried to Report Scientific Misconduct. How Did It Go?”</span> 2021. <a href="http://crystalprisonzone.blogspot.com/2021/01/i-tried-to-report-scientific-misconduct.html">http://crystalprisonzone.blogspot.com/2021/01/i-tried-to-report-scientific-misconduct.html</a>.
</div>
<div id="ref-James2021-mw" class="csl-entry" role="listitem">
James, Gareth Michael, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. <em>An Introduction to Statistical Learning: With Applications in <span>R</span></em>. Springer Nature.
</div>
<div id="ref-Janz2016-bt" class="csl-entry" role="listitem">
Janz, Nicole. 2016. <span>“Bringing the Gold Standard into the Classroom: Replication in University Teaching.”</span> <em>Int Stud Perspect</em> 17 (4): 392–407.
</div>
<div id="ref-John2012-vj" class="csl-entry" role="listitem">
John, Leslie K, George Loewenstein, and Drazen Prelec. 2012. <span>“Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling.”</span> <em>Psychol. Sci.</em> 23 (5): 524–32.
</div>
<div id="ref-Kaplan2015-fj" class="csl-entry" role="listitem">
Kaplan, Robert M, and Veronica L Irvin. 2015. <span>“Likelihood of Null Effects of Large <span>NHLBI</span> Clinical Trials Has Increased over Time.”</span> <em>PLoS One</em> 10 (8).
</div>
<div id="ref-Key2016-vr" class="csl-entry" role="listitem">
Key, Ellen M. 2016. <span>“How Are We Doing? Data Access and Replication in Political Science.”</span> <em>PS Polit. Sci. Polit.</em> 49 (02): 268–72.
</div>
<div id="ref-Kim2021-prereg" class="csl-entry" role="listitem">
Kim, D. G., Joshua Byun, and Sichen Li. 2021. <span>“Foreign Policy Revisionism in the Era of COVID-19: Theory and Evidence from Chinese Public Opinion.”</span> <a href="https://osf.io/r9dn7">https://osf.io/r9dn7</a>.
</div>
<div id="ref-King1990-hy" class="csl-entry" role="listitem">
King, Gary. 1990. <span>“On Political Methodology.”</span> <em>Polit. Anal.</em> 2: 1–29.
</div>
<div id="ref-king1995replication" class="csl-entry" role="listitem">
———. 1995. <span>“Replication, Replication.”</span> <em>PS: Political Science &amp; Politics</em> 28 (3): 444–52.
</div>
<div id="ref-Klein2014-sm" class="csl-entry" role="listitem">
Klein, Richard A, Kate A Ratliff, Michelangelo Vianello, Reginald B Adams Jr, Štěpán Bahnı́k, Michael J Bernstein, Konrad Bocian, et al. 2014. <span>“Investigating Variation in Replicability: A <span>‘Many Labs’</span> Replication Project.”</span> <em>Soc. Psychol.</em> 45 (3): 142–52.
</div>
<div id="ref-Klein2018-gr" class="csl-entry" role="listitem">
Klein, Richard A, Michelangelo Vianello, Fred Hasselman, Byron G Adams, Reginald B Adams, Sinan Alper, Mark Aveyard, et al. 2018. <span>“Many Labs 2: Investigating Variation in Replicability Across Samples and Settings.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (4): 443–90.
</div>
<div id="ref-Kristal2020-rh" class="csl-entry" role="listitem">
Kristal, Ariella S, Ashley V Whillans, Max H Bazerman, Francesca Gino, Lisa L Shu, Nina Mazar, and Dan Ariely. 2020. <span>“Signing at the Beginning Versus at the End Does Not Decrease Dishonesty.”</span> <em>Proc. Natl. Acad. Sci. U. S. A.</em> 117 (13): 7103–7.
</div>
<div id="ref-LaCour2014-ul" class="csl-entry" role="listitem">
LaCour, Michael J, and Donald P Green. 2014. <span>“Political Science. When Contact Changes Minds: An Experiment on Transmission of Support for Gay Equality.”</span> <em>Science</em> 346 (6215): 1366–69.
</div>
<div id="ref-Lakens2019-lt" class="csl-entry" role="listitem">
Lakens, Daniel. 2019. <span>“The Value of Preregistration for Psychological Science: A Conceptual Analysis.”</span> <em>PsyArXiv</em>.
</div>
<div id="ref-Lenz2021-kk" class="csl-entry" role="listitem">
Lenz, Gabriel S, and Alexander Sahn. 2021. <span>“Achieving Statistical Significance with Control Variables and Without Transparency.”</span> <em>Polit. Anal.</em> 29 (3): 356–69.
</div>
<div id="ref-Lupia2014-mn" class="csl-entry" role="listitem">
Lupia, Arthur, and Colin Elman. 2014. <span>“Openness in Political Science: Data Access and Research Transparency: Introduction.”</span> <em>PS Polit. Sci. Polit.</em> 47 (1): 19–42.
</div>
<div id="ref-monroe2018rush" class="csl-entry" role="listitem">
Monroe, Kristen Renwick. 2018. <span>“The Rush to Transparency: DA-RT and the Potential Dangers for Qualitative Research.”</span> <em>Perspectives on Politics</em> 16 (1): 141–48.
</div>
<div id="ref-NAP26308" class="csl-entry" role="listitem">
National Academies of Sciences, Engineering, and Medicine. 2021. <em>Developing a Toolkit for Fostering Open Science Practices: Proceedings of a Workshop</em>. Edited by Thomas Arrison, Jennifer Saunders, and Emi Kameyama. Washington, DC: The National Academies Press. <a href="https://www.nap.edu/catalog/26308/developing-a-toolkit-for-fostering-open-science-practices-proceedings-of">https://www.nap.edu/catalog/26308/developing-a-toolkit-for-fostering-open-science-practices-proceedings-of</a>.
</div>
<div id="ref-Leif2021-fo" class="csl-entry" role="listitem">
Nelson, Leif D, Uri Simonsohn, and Joseph P Simmons. 2021. <span>“[98] Evidence of Fraud in an Influential Field Experiment about Dishonesty.”</span> <a href="http://datacolada.org/98" class="uri">http://datacolada.org/98</a>.
</div>
<div id="ref-Neumayer2017-hc" class="csl-entry" role="listitem">
Neumayer, Eric, and Thomas Plümper. 2017. <em>Robustness Tests for Quantitative Research</em>. Cambridge University Press.
</div>
<div id="ref-Nosek2018-an" class="csl-entry" role="listitem">
Nosek, Brian A, Charles R Ebersole, Alexander C DeHaven, and David T Mellor. 2018. <span>“The Preregistration Revolution.”</span> <em>Proc. Natl. Acad. Sci. U. S. A.</em> 115 (11): 2600–2606.
</div>
<div id="ref-nosek2020replication" class="csl-entry" role="listitem">
Nosek, Brian A, and Timothy M Errington. 2020. <span>“What Is Replication?”</span> <em>PLoS Biology</em> 18 (3): e3000691.
</div>
<div id="ref-Nuijten2016-dp" class="csl-entry" role="listitem">
Nuijten, Michèle B, Chris H J Hartgerink, Marcel A L M van Assen, Sacha Epskamp, and Jelte M Wicherts. 2016. <span>“The Prevalence of Statistical Reporting Errors in Psychology (1985–2013).”</span> <em>Behav. Res. Methods</em> 48 (4): 1205–26.
</div>
<div id="ref-obels2020analysis" class="csl-entry" role="listitem">
Obels, Pepijn, Daniel Lakens, Nicholas A Coles, Jaroslav Gottfried, and Seth A Green. 2020. <span>“Analysis of Open Data and Computational Reproducibility in Registered Reports in Psychology.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 3 (2): 229–37.
</div>
<div id="ref-Ofosu_2020-xo" class="csl-entry" role="listitem">
Ofosu, George K, and Daniel N Posner. 2020. <span>“Pre-Analysis Plans: An Early Stocktaking.”</span> <em>Perspectives on Politics</em>, 1–17.
</div>
<div id="ref-Open_Science_Collaboration2015-gk" class="csl-entry" role="listitem">
Open Science Collaboration. 2015. <span>“<span>PSYCHOLOGY</span>. Estimating the Reproducibility of Psychological Science.”</span> <em>Science</em> 349 (6251): aac4716.
</div>
<div id="ref-dialogue-dart" class="csl-entry" role="listitem">
<span>“Perspectives on DA-RT.”</span> n.d. <a href="https://dialogueondart.org/perspectives-on-da-rt/">https://dialogueondart.org/perspectives-on-da-rt/</a>.
</div>
<div id="ref-YouthSoc_2020-ut" class="csl-entry" role="listitem">
<span>“Retraction Notice.”</span> 2020. <em>Youth Soc.</em> 52 (2): 308–8.
</div>
<div id="ref-Rhys2020-uc" class="csl-entry" role="listitem">
Rhys, Hefin. 2020. <em>Machine Learning with r, the Tidyverse, and Mlr</em>. Simon; Schuster.
</div>
<div id="ref-Scheel2021-wa" class="csl-entry" role="listitem">
Scheel, Anne M, Mitchell R M J Schijen, and Daniël Lakens. 2021. <span>“An Excess of Positive Results: Comparing the Standard Psychology Literature with Registered Reports.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 4 (2).
</div>
<div id="ref-Shapin1995-lj" class="csl-entry" role="listitem">
Shapin, Steven. 1995. <span>“Trust, Honesty, and the Authority of Science.”</span> In <em>Society’s Choices: Social and Ethical Decision Making in Biomedicine</em>, edited by Ruth Ellen Bulger, Elizabeth Meyer Bobby, and Harvey V Fineberg, 388–408. National Academy Press.
</div>
<div id="ref-Shapin2018-ir" class="csl-entry" role="listitem">
———. 2018. <em>The Scientific Revolution</em>. University of Chicago Press.
</div>
<div id="ref-Shu2012-nr" class="csl-entry" role="listitem">
Shu, Lisa L, Nina Mazar, Francesca Gino, Dan Ariely, and Max H Bazerman. 2012. <span>“Signing at the Beginning Makes Ethics Salient and Decreases Dishonest Self-Reports in Comparison to Signing at the End.”</span> <em>Proceedings of the National Academy of Sciences</em>.
</div>
<div id="ref-Simmons2011-tp" class="csl-entry" role="listitem">
Simmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011. <span>“False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.”</span> <em>Psychol. Sci.</em> 22 (11): 1359–66.
</div>
<div id="ref-Simmons2017-rs" class="csl-entry" role="listitem">
Simmons, Joseph P, and Uri Simonsohn. 2017. <span>“Power Posing: <span>P-Curving</span> the Evidence.”</span> <em>Psychol. Sci.</em> 28 (5): 687–93.
</div>
<div id="ref-Simmons2021-qm" class="csl-entry" role="listitem">
Simmons, Joseph, Leif Nelson, and Uri Simonsohn. 2021. <span>“Pre‐registration: Why and How.”</span> <em>J. Consum. Psychol.</em> 31 (1): 151–62.
</div>
<div id="ref-Simonsohn2013-fa" class="csl-entry" role="listitem">
Simonsohn, Uri. 2013. <span>“Just Post It: The Lesson from Two Cases of Fabricated Data Detected by Statistics Alone.”</span> <em>Psychol. Sci.</em> 24 (10): 1875–88.
</div>
<div id="ref-Simonsohn2014-qg" class="csl-entry" role="listitem">
Simonsohn, Uri, Leif D Nelson, and Joseph P Simmons. 2014. <span>“P-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results.”</span> <em>Perspect. Psychol. Sci.</em> 9 (6): 666–81.
</div>
<div id="ref-JETS_2015-mm" class="csl-entry" role="listitem">
Statement, Journal Editors’ Transparency. 2015. <span>“Data Access and Research Transparency (<span>DA-RT)</span>: A Joint Statement by Political Science Journal Editors.”</span> <em>Political Science Research and Methods</em> 3 (3): 421–21.
</div>
<div id="ref-Stockemer2018-qg" class="csl-entry" role="listitem">
Stockemer, Daniel, Sebastian Koehler, and Tobias Lentz. 2018. <span>“Data Access, Transparency, and Replication: New Insights from the Political Behavior Literature.”</span> <em>PS Polit. Sci. Polit.</em> 51 (4): 799–803.
</div>
<div id="ref-Tong2019-as" class="csl-entry" role="listitem">
Tong, Christopher. 2019. <span>“Statistical Inference Enables Bad Science; Statistical Thinking Enables Good Science.”</span> <em>Am. Stat.</em> 73 (sup1): 246–61.
</div>
</div>
<!-- \newpage -->
<!-- \singlespacing -->
<!-- ```{r child = 'Appendix.Rmd'} -->
<!-- ``` -->
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>PhD Candidate, School of Politics and International Relations, Australian National University. Corresponding author. Email: bermond.scoggins@anu.edu.au.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>PhD Candidate, School of Politics and International Relations, Australian National University. Senior Research Associate, School of Social Sciences, University of Mannheim. Email: matthew.peter.robertson@uni-mannheim.de<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>PhD Candidate, School of Politics and International Relations, Australian National University. Corresponding author. Email: bermond.scoggins@anu.edu.au.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>PhD Candidate, School of Politics and International Relations, Australian National University. Senior Research Associate, School of Social Sciences, University of Mannheim. Email: matthew.peter.robertson@uni-mannheim.de<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>PhD Candidate, School of Politics and International Relations, Australian National University. Corresponding author. Email: bermond.scoggins@anu.edu.au.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>PhD Candidate, School of Politics and International Relations, Australian National University. Senior Research Associate, School of Social Sciences, University of Mannheim. Email: matthew.peter.robertson@uni-mannheim.de<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="citation" data-cites="Key2016-vr">Key (<a href="#ref-Key2016-vr" role="doc-biblioref">2016</a>)</span> analyses 586 articles in six top political science and international relations journals – some of which have already adopted compulsory data availability policies – for 2014 and 2015. <span class="citation" data-cites="Stockemer2018-qg">Stockemer, Koehler, and Lentz (<a href="#ref-Stockemer2018-qg" role="doc-biblioref">2018</a>)</span> analyse data availability in the articles of three journals in 2015. Grossman and Pedahzur <span class="citation" data-cites="Grossman2020-yv">(<a href="#ref-Grossman2020-yv" role="doc-biblioref">2020, 1</a>)</span> analyze 92 articles published in the Fall 2019 issues of six journals and argue that the field is now approaching a “replicability utopia”.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>A complete list of the journals can be found in the appendix.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The three journals are , , were analysed.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>The six journals analysed were , , , , , .<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Summaries of these debates can be found in Lupia and Elman <span class="citation" data-cites="Lupia2014-mn">(<a href="#ref-Lupia2014-mn" role="doc-biblioref">2014</a>)</span> and on the Dialogue on DART website <span class="citation" data-cites="dialogue-dart">(<a href="#ref-dialogue-dart" role="doc-biblioref"><span>“Perspectives on DA-RT,”</span> n.d.</a>)</span>.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>We discuss these issues further in the results section.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>For instance, Lenz and Sahn <span class="citation" data-cites="Lenz2021-kk">(<a href="#ref-Lenz2021-kk" role="doc-biblioref">2021</a>)</span> find that over 30% of observational studies published in the rely on suppression effects to achieve statistical significance. Being able to determine the influence of suppression effects requires access to the original data.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>As some journals publish both political science and international relations articles, the top 100 journals in each category overlapped.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>A more complete discussion of the choice of this field is found in the <code>./rr_code/rr_crossref_date_field_choice_analysis.R</code> file, which also shows the unintentional omission of seven journals.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>A custom function was preferable to existing text analysis libraries like <code>quanteda</code> because of our need to capture regular expressions and asterisks.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>As an additional robustness check to predict open data and statistical inference papers, we estimated a series of bivariate logistic regressions using the same DFMs. The predicted probability plots can be found in the appendix. These plots give a lower estimate than the machine learning models, though they are in the same broad range. We also attempted to use the Claude 3 Haiku model by Anthropic, but discontinued this experiment due to time and resource constraints, as detailed in the letter to reviewers in the replication materials.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>Terms like “replication data” are used in political science to refer to computational reproducibility materials such as open data and code.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>Gary King illustrated that by 1988 almost half of publications in the American Political Science Review were quantitative.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>The cutoff was established to focus on journals who publish more quantitative papers and for ease of viewing – the graph with all 158 journals with at least one statistical inference paper is very large and is located in the appendix.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>The journals with over 50% open data are the , the , the , , , , , , , , and . Those with over 20% open data include the aforementioned journals as well as , , , , and .<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>A total of 27 journals signed the DA-RT statement. The majority of these journals publish quantitative research (as can be seen in Figure 2). Note that there are actually 20 DA-RT signatory journals in our dataset, but four of them have an insignificant number of statistical inference publications and so we omit them from the analysis.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>Journals like the require authors to disclose a preregistration report or justify why they did not preregister their experiment.<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>For experiments, we acknowledge that these are by no means definitive criteria on which to judge the trustworthiness of a paper or finding. These practices should accompany efforts to build confidence in a finding through direct and conceptual replications.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Bermond Scoggins at 2024-04-08 13:24:18 +1000 


%% Saved with string encoding Unicode (UTF-8) 



@article{monroe2018rush,
	author = {Monroe, Kristen Renwick},
	date-added = {2024-04-08 13:24:12 +1000},
	date-modified = {2024-04-08 13:24:12 +1000},
	journal = {Perspectives on Politics},
	number = {1},
	pages = {141--148},
	publisher = {Cambridge University Press},
	title = {The rush to transparency: DA-RT and the potential dangers for qualitative research},
	volume = {16},
	year = {2018}}

@article{nosek2020replication,
	author = {Nosek, Brian A and Errington, Timothy M},
	date-added = {2024-04-08 13:17:08 +1000},
	date-modified = {2024-04-08 13:17:08 +1000},
	journal = {PLoS biology},
	number = {3},
	pages = {e3000691},
	publisher = {Public Library of Science San Francisco, CA USA},
	title = {What is replication?},
	volume = {18},
	year = {2020}}

@article{obels2020analysis,
	author = {Obels, Pepijn and Lakens, Daniel and Coles, Nicholas A and Gottfried, Jaroslav and Green, Seth A},
	date-added = {2024-04-08 12:40:29 +1000},
	date-modified = {2024-04-08 12:40:29 +1000},
	journal = {Advances in Methods and Practices in Psychological Science},
	number = {2},
	pages = {229--237},
	publisher = {Sage Publications Sage CA: Los Angeles, CA},
	title = {Analysis of open data and computational reproducibility in registered reports in psychology},
	volume = {3},
	year = {2020}}

@article{king1995replication,
	author = {King, Gary},
	date-added = {2024-04-07 19:16:49 +1000},
	date-modified = {2024-04-07 19:16:49 +1000},
	journal = {PS: Political Science \& Politics},
	number = {3},
	pages = {444--452},
	publisher = {Cambridge University Press},
	title = {Replication, replication},
	volume = {28},
	year = {1995}}

@article{hardwicke2021analytic,
	author = {Hardwicke, Tom E and Bohn, Manuel and MacDonald, Kyle and Hembacher, Emily and Nuijten, Mich{\`e}le B and Peloquin, Benjamin N and DeMayo, Benjamin E and Long, Bria and Yoon, Erica J and Frank, Michael C},
	date-added = {2024-02-20 17:18:44 +0800},
	date-modified = {2024-02-20 17:18:44 +0800},
	journal = {Royal Society open science},
	number = {1},
	pages = {201494},
	publisher = {The Royal Society},
	title = {Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: An observational study},
	volume = {8},
	year = {2021}}

@article{hardwicke2018data,
	author = {Hardwicke, Tom E and Mathur, Maya B and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C and Kidwell, Mallory C and Hofelich Mohr, Alicia and Clayton, Elizabeth and Yoon, Erica J and Henry Tessler, Michael and others},
	date-added = {2024-02-20 17:11:48 +0800},
	date-modified = {2024-02-20 17:11:48 +0800},
	journal = {Royal Society open science},
	number = {8},
	pages = {180448},
	publisher = {The Royal Society},
	title = {Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal Cognition},
	volume = {5},
	year = {2018}}

@url{Kim2021-prereg,
	author = {Kim, D.G. and Byun, Joshua and Li, Sichen},
	date-added = {2022-06-15 19:58:31 +0600},
	date-modified = {2022-06-15 20:09:10 +0600},
	title = {Foreign Policy Revisionism in the Era of COVID-19: Theory and Evidence from Chinese Public Opinion},
	url = {https://osf.io/r9dn7},
	year = {2021},
	bdsk-url-1 = {https://osf.io/r9dn7}}

@article{byun2021geopolitical,
	author = {Byun, Joshua and Kim, DG and Li, Sichen},
	date-added = {2022-06-15 19:04:43 +0600},
	date-modified = {2022-06-15 19:04:43 +0600},
	journal = {Political Science Quarterly},
	number = {4},
	pages = {641--665},
	publisher = {Wiley Online Library},
	title = {The Geopolitical Consequences of COVID-19: Assessing Hawkish Mass Opinion in China},
	volume = {136},
	year = {2021}}

@electronic{JCR2020,
	author = {Clarivate},
	date-added = {2022-01-16 18:02:41 +1100},
	date-modified = {2022-01-16 18:04:17 +1100},
	title = {Journal Citation Reports},
	year = {2020}}

@article{Simmons2017-rs,
	author = {Simmons, Joseph P and Simonsohn, Uri},
	date-added = {2021-12-28 17:53:52 +1100},
	date-modified = {2021-12-28 17:53:52 +1100},
	journal = {Psychol. Sci.},
	language = {en},
	month = may,
	number = 5,
	pages = {687--693},
	title = {Power Posing: {P-Curving} the Evidence},
	volume = 28,
	year = 2017}

@article{Errington2021-ra,
	abstract = {Replicability is an important feature of scientific research, but
              aspects of contemporary research culture, such as an emphasis on
              novelty, can make replicability seem less important than it
              should be. The Reproducibility Project: Cancer Biology was set up
              to provide evidence about the replicability of preclinical
              research in cancer biology by repeating selected experiments from
              high-impact papers. A total of 50 experiments from 23 papers were
              repeated, generating data about the replicability of a total of
              158 effects. Most of the original effects were positive effects
              (136), with the rest being null effects (22). A majority of the
              original effect sizes were reported as numerical values (117),
              with the rest being reported as representative images (41). We
              employed seven methods to assess replicability, and some of these
              methods were not suitable for all the effects in our sample. One
              method compared effect sizes: for positive effects, the median
              effect size in the replications was 85\% smaller than the median
              effect size in the original experiments, and 92\% of replication
              effect sizes were smaller than the original. The other methods
              were binary - the replication was either a success or a failure -
              and five of these methods could be used to assess both positive
              and null effects when effect sizes were reported as numerical
              values. For positive effects, 40\% of replications (39/97)
              succeeded according to three or more of these five methods, and
              for null effects 80\% of replications (12/15) were successful on
              this basis; combining positive and null effects, the success rate
              was 46\% (51/112). A successful replication does not definitively
              confirm an original finding or its theoretical interpretation.
              Equally, a failure to replicate does not disconfirm a finding,
              but it does suggest that additional investigation is needed to
              establish its reliability.},
	author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	date-added = {2021-12-27 21:58:20 +1100},
	date-modified = {2021-12-27 21:58:20 +1100},
	journal = {Elife},
	keywords = {Reproducibility Project: Cancer Biology; cancer biology; computational biology; credibility; human; meta-analysis; mouse; replication; reproducibility; reproducibility in cancer biology; systems biology; transparency},
	language = {en},
	month = dec,
	title = {Investigating the replicability of preclinical cancer biology},
	volume = 10,
	year = 2021}

@unpublished{Lakens2019-lt,
	abstract = {For over two centuries researchers have been criticized for using
              research practices that makes it easier to present data in line
              with what they wish to be true. With the rise of the internet it
              has become easier to preregister the theoretical and empirical
              basis for predictions, the experimental design, the materials,
              and the analysis code. Whether the practice of preregistration is
              valuable depends on your philosophy of science. Here, I provide a
              conceptual analysis of the value of preregistration for
              psychological science from an error statistical philosophy (Mayo,
              2018). Preregistration has the goal to allow others to
              transparently evaluate the capacity of a test to falsify a
              prediction, or the severity of a test. Researchers who aim to
              test predictions with severity should find value in the practice
              of preregistration. I differentiate the goal of preregistration
              from positive externalities, discuss how preregistration itself
              does not make a study better or worse compared to a
              non-preregistered study, and highlight the importance of
              evaluating the usefulness of a tool such as preregistration based
              on an explicit consideration of your philosophy of science.},
	author = {Lakens, Daniel},
	date-added = {2021-12-27 19:56:58 +1100},
	date-modified = {2021-12-27 19:56:58 +1100},
	journal = {PsyArXiv},
	month = nov,
	title = {The value of preregistration for psychological science: A conceptual analysis},
	year = 2019}

@article{Ofosu_2020-xo,
	abstract = {Pre-analysis plans (PAPs) have been championed as a solution to
               the problem of research credibility, but without any evidence
               that PAPs actually bolster the credibility of research. We
               analyze a representative sample of 195 PAPs registered on the
               Evidence in Governance and Politics (EGAP) and American Economic
               Association (AEA) registration platforms to assess whether PAPs
               registered in the early days of pre-registration (2011--2016)
               were sufficiently clear, precise, and comprehensive to achieve
               their objective of preventing ``fishing'' and reducing the scope
               for post-hoc adjustment of research hypotheses. We also analyze
               a subset of ninety-three PAPs from projects that resulted in
               publicly available papers to ascertain how faithfully they
               adhere to their pre-registered specifications and hypotheses. We
               find significant variation in the extent to which PAPs
               registered during this period accomplished the goals they were
               designed to achieve. We discuss these findings in light of both
               the costs and benefits of pre-registration, showing how our
               results speak to the various arguments that have been made in
               support of and against PAPs. We also highlight the norms and
               institutions that will need to be strengthened to augment the
               power of PAPs to improve research credibility and to create
               incentives for researchers to invest in both producing and
               policing them.},
	author = {Ofosu, George K and Posner, Daniel N},
	date-added = {2021-12-27 18:22:51 +1100},
	date-modified = {2021-12-27 18:23:15 +1100},
	journal = {Perspectives on Politics},
	pages = {1--17},
	publisher = {Cambridge University Press},
	title = {Pre-Analysis Plans: An Early Stocktaking},
	year = {2020}}

@article{Errington2021-zg,
	abstract = {We conducted the Reproducibility Project: Cancer Biology to
              investigate the replicability of preclinical research in cancer
              biology. The initial aim of the project was to repeat 193
              experiments from 53 high-impact papers, using an approach in
              which the experimental protocols and plans for data analysis had
              to be peer reviewed and accepted for publication before
              experimental work could begin. However, the various barriers and
              challenges we encountered while designing and conducting the
              experiments meant that we were only able to repeat 50 experiments
              from 23 papers. Here we report these barriers and challenges.
              First, many original papers failed to report key descriptive and
              inferential statistics: the data needed to compute effect sizes
              and conduct power analyses was publicly accessible for just 4 of
              193 experiments. Moreover, despite contacting the authors of the
              original papers, we were unable to obtain these data for 68\% of
              the experiments. Second, none of the 193 experiments were
              described in sufficient detail in the original paper to enable us
              to design protocols to repeat the experiments, so we had to seek
              clarifications from the original authors. While authors were
              extremely or very helpful for 41\% of experiments, they were
              minimally helpful for 9\% of experiments, and not at all helpful
              (or did not respond to us) for 32\% of experiments. Third, once
              experimental work started, 67\% of the peer-reviewed protocols
              required modifications to complete the research and just 41\% of
              those modifications could be implemented. Cumulatively, these
              three factors limited the number of experiments that could be
              repeated. This experience draws attention to a basic and
              fundamental concern about replication - it is hard to assess
              whether reported findings are credible.},
	author = {Errington, Timothy M and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	date-added = {2021-12-27 18:22:51 +1100},
	date-modified = {2021-12-27 18:22:51 +1100},
	journal = {Elife},
	keywords = {Reproducibility Project: Cancer Biology; cancer biology; human; mouse; open data; open science; preregistration; replication; reproducibility},
	language = {en},
	month = dec,
	title = {Challenges for assessing replicability in preclinical cancer biology},
	volume = 10,
	year = 2021}

@article{Kaplan2015-fj,
	abstract = {BACKGROUND: We explore whether the number of null results in
               large National Heart Lung, and Blood Institute (NHLBI) funded
               trials has increased over time. METHODS: We identified all large
               NHLBI supported RCTs between 1970 and 2012 evaluating drugs or
               dietary supplements for the treatment or prevention of
               cardiovascular disease. Trials were included if direct costs
               >\$500,000/year, participants were adult humans, and the primary
               outcome was cardiovascular risk, disease or death. The 55 trials
               meeting these criteria were coded for whether they were
               published prior to or after the year 2000, whether they
               registered in clinicaltrials.gov prior to publication, used
               active or placebo comparator, and whether or not the trial had
               industry co-sponsorship. We tabulated whether the study reported
               a positive, negative, or null result on the primary outcome
               variable and for total mortality. RESULTS: 17 of 30 studies
               (57\%) published prior to 2000 showed a significant benefit of
               intervention on the primary outcome in comparison to only 2
               among the 25 (8\%) trials published after 2000 ($\chi$2=12.2,df=
               1, p=0.0005). There has been no change in the proportion of
               trials that compared treatment to placebo versus active
               comparator. Industry co-sponsorship was unrelated to the
               probability of reporting a significant benefit. Pre-registration
               in clinical trials.gov was strongly associated with the trend
               toward null findings. CONCLUSIONS: The number NHLBI trials
               reporting positive results declined after the year 2000.
               Prospective declaration of outcomes in RCTs, and the adoption of
               transparent reporting standards, as required by
               clinicaltrials.gov, may have contributed to the trend toward
               null findings.},
	author = {Kaplan, Robert M and Irvin, Veronica L},
	copyright = {https://creativecommons.org/publicdomain/zero/1.0/},
	date-added = {2021-11-20 15:08:02 +1100},
	date-modified = {2022-01-16 18:13:01 +1100},
	journal = {PLoS One},
	language = {en},
	month = aug,
	number = 8,
	publisher = {Public Library of Science (PLoS)},
	title = {Likelihood of null effects of large {NHLBI} clinical trials has increased over time},
	volume = 10,
	year = 2015}

@book{NAP26308,
	abstract = {The National Academies Roundtable on Aligning Incentives for Open Science, established in 2019, has taken on an important role in addressing issues with open science. The roundtable convenes critical stakeholders to discuss the effectiveness of current incentives for adopting open science practices, current barriers of all types, and ways to move forward in order to align reward structures and institutional values. The Roundtable convened a virtual public workshop on fostering open science practices on November 5, 2020. The broad goal of the workshop was to identify paths to growing the nascent coalition of stakeholders committed to reenvisioning credit\/reward systems (e.g., academic hiring, tenure and promotion, and grants)to fully incentivize open science practices. The workshop explored the information and resource needs of researchers, research institutions, government agencies, philanthropies, professional societies, and other stakeholders interested in further supporting and implementing open science practices.  This publication summarizes the presentations and discussion of the workshop.},
	address = {Washington, DC},
	author = {National Academies of Sciences, Engineering, and Medicine},
	date-modified = {2022-01-16 18:14:19 +1100},
	editor = {Thomas Arrison and Jennifer Saunders and Emi Kameyama},
	isbn = {978-0-309-09361-3},
	publisher = {The National Academies Press},
	title = {Developing a Toolkit for Fostering Open Science Practices: Proceedings of a Workshop},
	url = {https://www.nap.edu/catalog/26308/developing-a-toolkit-for-fostering-open-science-practices-proceedings-of},
	year = 2021,
	bdsk-url-1 = {https://www.nap.edu/catalog/26308/developing-a-toolkit-for-fostering-open-science-practices-proceedings-of},
	bdsk-url-2 = {https://doi.org/10.17226/26308}}

@article{Baggerly-2009,
	author = {Keith A. Baggerly and Kevin R. Coombes},
	date-modified = {2022-01-16 18:10:42 +1100},
	doi = {10.1214/09-AOAS291},
	journal = {The Annals of Applied Statistics},
	keywords = {forensic bioinformatics, microarrays, reproducibility},
	number = {4},
	pages = {1309 -- 1334},
	publisher = {Institute of Mathematical Statistics},
	title = {{Deriving chemosensitivity from cell lines: Forensic bioinformatics and reproducible research in high-throughput biology}},
	volume = {3},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1214/09-AOAS291}}

@article{Roukema2021-xq,
	author = {Roukema, Boudewijn F},
	doi = {10.7717/peerj.11856},
	issn = {2167-8359},
	journal = {PeerJ},
	keywords = {COVID-19; Data validation; Poisson point process; SARS-CoV-2},
	language = {en},
	month = aug,
	pages = {e11856},
	pmc = {PMC8404575},
	pmid = {34532156},
	title = {Anti-clustering in the national {SARS-CoV-2} daily infection counts},
	url = {http://dx.doi.org/10.7717/peerj.11856},
	volume = 9,
	year = 2021,
	bdsk-url-1 = {http://dx.doi.org/10.7717/peerj.11856}}

@incollection{Shapin1995-lj,
	author = {Shapin, Steven},
	booktitle = {Society's Choices: Social and Ethical Decision Making in Biomedicine},
	date-modified = {2022-01-16 18:11:57 +1100},
	editor = {Bulger, Ruth Ellen and Bobby, Elizabeth Meyer and Fineberg, Harvey V},
	pages = {388--408},
	publisher = {National Academy Press},
	title = {Trust, Honesty, and the Authority of Science},
	year = 1995,
	bdsk-url-1 = {https://philpapers.org/rec/SHATHA}}

@book{Shapin2018-ir,
	author = {Shapin, Steven},
	date-modified = {2022-01-16 18:12:03 +1100},
	isbn = {9780226398488},
	language = {en},
	month = nov,
	publisher = {University of Chicago Press},
	title = {The Scientific Revolution},
	year = 2018,
	bdsk-url-1 = {https://play.google.com/store/books/details?id=CttwDwAAQBAJ}}

@book{Rhys2020-uc,
	author = {Rhys, Hefin},
	date-modified = {2022-01-16 18:14:40 +1100},
	isbn = {9781638350170},
	language = {en},
	month = mar,
	publisher = {Simon and Schuster},
	title = {Machine Learning with R, the tidyverse, and mlr},
	year = 2020,
	bdsk-url-1 = {https://play.google.com/store/books/details?id=qjszEAAAQBAJ}}

@book{James2021-mw,
	author = {James, Gareth Michael and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	date-modified = {2022-01-16 18:11:08 +1100},
	isbn = {9781071614181},
	language = {en},
	publisher = {Springer Nature},
	title = {An Introduction to Statistical Learning: With Applications in {R}},
	year = 2021,
	bdsk-url-1 = {https://play.google.com/store/books/details?id=5dQ6EAAAQBAJ}}

@book{Cristianini2000-bu,
	author = {Cristianini, Nello and Shawe-Taylor, John},
	date-modified = {2021-12-28 17:44:07 +1100},
	isbn = {9780521780193},
	language = {en},
	month = mar,
	publisher = {Cambridge University Press},
	title = {An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
	year = 2000,
	bdsk-url-1 = {https://play.google.com/store/books/details?id=_PXJn_cxv0AC}}

@book{Merton1973-hd,
	author = {Merton, Robert K},
	isbn = {9780226520926},
	language = {en},
	publisher = {University of Chicago Press},
	title = {The Sociology of Science: Theoretical and Empirical Investigations},
	url = {https://play.google.com/store/books/details?id=zPvcHuUMEMwC},
	year = 1973,
	bdsk-url-1 = {https://play.google.com/store/books/details?id=zPvcHuUMEMwC}}

@electronic{Hilgard_2021_wb,
	author = {Hilgard, Joseph},
	date-added = {2021-09-30 03:25:11 +1000},
	date-modified = {2021-09-30 03:26:11 +1000},
	title = {I tried to report scientific misconduct. How did it go?},
	url = {http://crystalprisonzone.blogspot.com/2021/01/i-tried-to-report-scientific-misconduct.html},
	year = {2021},
	bdsk-url-1 = {http://crystalprisonzone.blogspot.com/2021/01/i-tried-to-report-scientific-misconduct.html}}

@techreport{Hilgard_2020-lg,
	abstract = {Hosted on the Open Science Framework},
	author = {Hilgard, Joseph},
	date-added = {2021-09-30 03:00:17 +1000},
	date-modified = {2021-09-30 03:47:00 +1000},
	language = {en},
	publisher = {OSF},
	title = {Curious features of data in Zhang et al. (2019)},
	year = {2020}}

@article{YouthSoc_2020-ut,
	date-added = {2021-09-30 03:00:17 +1000},
	date-modified = {2021-09-30 04:25:05 +1000},
	journal = {Youth Soc.},
	month = mar,
	number = 2,
	pages = {308--308},
	publisher = {SAGE Publications Inc},
	title = {Retraction Notice},
	volume = 52,
	year = 2020}

@article{Nuijten2016-dp,
	author = {Nuijten, Mich{\`e}le B and Hartgerink, Chris H J and van Assen, Marcel A L M and Epskamp, Sacha and Wicherts, Jelte M},
	date-added = {2021-09-30 02:15:42 +1000},
	date-modified = {2021-09-30 02:15:42 +1000},
	journal = {Behav. Res. Methods},
	language = {en},
	month = dec,
	number = 4,
	pages = {1205--1226},
	publisher = {Springer Science and Business Media LLC},
	title = {The prevalence of statistical reporting errors in psychology (1985--2013)},
	volume = 48,
	year = 2016}

@webpage{intinterac2021,
	author = {International Interactions},
	date-added = {2021-09-29 20:43:43 +1000},
	date-modified = {2021-09-30 03:50:08 +1000},
	url = {https://www.tandfonline.com/action/authorSubmission?show=instructions&journalCode=gini20},
	bdsk-url-1 = {https://www.tandfonline.com/action/authorSubmission?show=instructions&journalCode=gini20}}

@electronic{dialogue-dart,
	date-added = {2021-09-28 15:53:49 +1000},
	date-modified = {2021-09-28 16:41:26 +1000},
	title = {Perspectives on DA-RT},
	url = {https://dialogueondart.org/perspectives-on-da-rt/},
	bdsk-url-1 = {https://dialogueondart.org/perspectives-on-da-rt/}}

@article{JETS_2015-mm,
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS2049847015000448/resource/name/firstPage-S2049847015000448a.jpg},
	author = {Journal Editors' Transparency Statement},
	date-added = {2021-09-28 15:39:35 +1000},
	date-modified = {2021-09-28 15:41:20 +1000},
	journal = {Political Science Research and Methods},
	month = sep,
	number = 3,
	pages = {421--421},
	publisher = {Cambridge University Press},
	title = {Data Access and Research Transparency ({DA-RT)}: A Joint Statement by Political Science Journal Editors},
	volume = 3,
	year = 2015}

@article{Lupia2014-mn,
	abstract = {In 2012, the American Political Science Association (APSA)
               Council adopted new policies guiding data access and research
               transparency in political science. The policies appear as a
               revision to APSA's Guide to Professional Ethics in Political
               Science. The revisions were the product of an extended and broad
               consultation with a variety of APSA committees and the
               association's membership.},
	author = {Lupia, Arthur and Elman, Colin},
	date-added = {2021-09-28 15:33:26 +1000},
	date-modified = {2021-09-28 15:33:26 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = jan,
	number = 1,
	pages = {19--42},
	publisher = {Cambridge University Press},
	title = {Openness in Political Science: Data Access and Research Transparency: Introduction},
	volume = 47,
	year = 2014}

@article{Klein2014-sm,
	abstract = {[Correction Notice: An Erratum for this article was reported in
              Vol 50(3) of Social Psychology (see record 2019-34530-002). The
              article contained some errors. One line of code was incorrect in
              the script that generated results for Rugg (1941). Effectively,
              the authors failed to correctly invert two of the columns in
              Tables 2 and 3. The revised statistics do not alter the
              substantive conclusions for this effect (e.g., it remains a
              successful replication), however the correct effect size is much
              smaller and closer to the result reported in the original study.
              In addition, a typo was incorrect in Table 2, that led to the df
              and N reported for one of the anchoring studies to be slightly
              off. The corrections are included in the erratum. (PsycInfo
              Database Record (c) 2020 APA, all rights reserved)},
	author = {Klein, Richard A and Ratliff, Kate A and Vianello, Michelangelo and Adams, Jr, Reginald B and Bahn{\'\i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J and Bocian, Konrad and Brandt, Mark J and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A and Hovermale, James F and Hunt, S Jane and Huntsinger, Jeffrey R and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A and Barry Kappes, Heather and Krueger, Lacy E and Kurtz, Jaime and Levitan, Carmel A and Mallett, Robyn K and Morris, Wendy L and Nelson, Anthony J and Nier, Jason A and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M and Schmidt, Kathleen and Skorinko, Jeanine L and Smith, Robert and Steiner, Troy G and Storbeck, Justin and Van Swol, Lyn M and Thompson, Donna and van 't Veer, A E and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L and Woodzicka, Julie A and Nosek, Brian A},
	date-added = {2021-09-22 15:55:15 +1000},
	date-modified = {2021-09-22 15:55:15 +1000},
	journal = {Soc. Psychol.},
	number = 3,
	pages = {142--152},
	title = {Investigating variation in replicability: A ``many labs'' replication project},
	volume = 45,
	year = 2014}

@article{Klein2018-gr,
	abstract = {We conducted preregistered replications of 28 classic and
               contemporary published findings, with protocols that were peer
               reviewed in advance, to examine variation in effect magnitudes
               across samples and settings. Each protocol was administered to
               approximately half of 125 samples that comprised 15,305
               participants from 36 countries and territories. Using the
               conventional criterion of statistical significance (p < .05), we
               found that 15 (54\%) of the replications provided evidence of a
               statistically significant effect in the same direction as the
               original finding. With a strict significance criterion (p <
               .0001), 14 (50\%) of the replications still provided such
               evidence, a reflection of the extremely high-powered design.
               Seven (25\%) of the replications yielded effect sizes larger
               than the original ones, and 21 (75\%) yielded effect sizes
               smaller than the original ones. The median comparable Cohen?s ds
               were 0.60 for the original findings and 0.15 for the
               replications. The effect sizes were small (< 0.20) in 16 of the
               replications (57\%), and 9 effects (32\%) were in the direction
               opposite the direction of the original effect. Across settings,
               the Q statistic indicated significant heterogeneity in 11 (39\%)
               of the replication effects, and most of those were among the
               findings with the largest overall effect sizes; only 1 effect
               that was near zero in the aggregate showed significant
               heterogeneity according to this measure. Only 1 effect had a tau
               value greater than .20, an indication of moderate heterogeneity.
               Eight others had tau values near or slightly above .10, an
               indication of slight heterogeneity. Moderation tests indicated
               that very little heterogeneity was attributable to the order in
               which the tasks were performed or whether the tasks were
               administered in lab versus online. Exploratory comparisons
               revealed little heterogeneity between Western, educated,
               industrialized, rich, and democratic (WEIRD) cultures and less
               WEIRD cultures (i.e., cultures with relatively high and low
               WEIRDness scores, respectively). Cumulatively, variability in
               the observed effect sizes was attributable more to the effect
               being studied than to the sample or setting in which it was
               studied.},
	author = {Klein, Richard A and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G and Adams, Reginald B and Alper, Sinan and Aveyard, Mark and Axt, Jordan R and Babalola, Mayowa T and Bahn{\'\i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J and Berry, Daniel R and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E and Cheong, Winnee and Cicero, David C and Coen, Sharon and Coleman, Jennifer A and Collisson, Brian and Conway, Morgan A and Corker, Katherine S and Curran, Paul G and Cushman, Fiery and Dagona, Zubairu K and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R and Edlund, John E and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C and Ghoshal, Tanuka and Giessner, Steffen R and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E and Grahek, Ivan and Green, Eva G T and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L and Hall, Michael P and Heffernan, Marie E and Hicks, Joshua A and Houdek, Petr and Huntsinger, Jeffrey R and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, {\AA}se H and Jim{\'e}nez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A and Kamilo{\u g}lu, Roza G and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B and Levitan, Carmel A and Lewis, Neil A and Lins, Samuel and Lipsey, Nikolette P and Losee, Joy E and Maassen, Esther and Maitner, Angela T and Malingumu, Winfrida and Mallett, Robyn K and Marotta, Satia A and Me{\dj}edovi{\'c}, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L and Morris, Wendy L and Murphy, Sean C and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and P{\'e}rez-S{\'a}nchez, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M H and Rutchick, Abraham M and Saavedra, Patricio and Saeri, Alexander K and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D and Sekerdej, Maciej B and Sirlop{\'u}, David and Skorinko, Jeanine L M and Smith, Michael A and Smith-Castro, Vanessa and Smolders, Karin C H J and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G and Stouten, Jeroen and Street, Chris N H and Sundfelt, Oskar K and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C W and Tanzer, Norbert and Tear, Morgan J and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M and Ujhelyi, Adrienn and van Aert, Robbie C M and van Assen, Marcel A L M and van der Hulst, Marije and van Lange, Paul A M and van 't Veer, Anna Elisabeth and V{\'a}squez- Echeverr{\'\i}a, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P J and Vranka, Marek A and Welch, Cheryl and Wichman, Aaron L and Williams, Lisa A and Wood, Michael and Woodzicka, Julie A and Wronska, Marta K and Young, Liane and Zelenski, John M and Zhijia, Zeng and Nosek, Brian A},
	date-added = {2021-09-22 15:55:15 +1000},
	date-modified = {2021-09-22 15:55:15 +1000},
	journal = {Advances in Methods and Practices in Psychological Science},
	month = dec,
	number = 4,
	pages = {443--490},
	publisher = {SAGE Publications Inc},
	title = {Many Labs 2: Investigating Variation in Replicability Across Samples and Settings},
	volume = 1,
	year = 2018}

@unpublished{Claesen2019-th,
	abstract = {Doing research inevitably involves making numerous decisions that
              can influence research outcomes in such a way that it leads to
              overconfidence in statistical conclusions. One proposed method to
              increase the interpretability of a research finding is
              preregistration, which involves documenting analytic choices on a
              public, third-party repository prior to any influence by data. To
              investigate whether, in psychology, preregistration lives up to
              that potential, we focused on all articles published in
              Psychological Science with a preregistered badge between February
              2015 and November 2017, and assessed the adherence to their
              corresponding preregistration plans. We observed deviations from
              the plan in all studies, and, more importantly, in all but one
              study, at least one of these deviations was not fully disclosed.
              We discuss examples and possible explanations, and highlight good
              practices for preregistering research.},
	author = {Claesen, Aline and Gomes, Sara L B T and Tuerlinckx, Francis and Vanpaemel, Wolf},
	date-added = {2021-09-22 15:10:09 +1000},
	date-modified = {2021-09-22 15:10:09 +1000},
	month = may,
	title = {Preregistration: Comparing Dream to Reality},
	year = 2019}

@misc{Carney2015-sy,
	author = {Carney, Dana},
	date-added = {2021-09-22 03:53:45 +1000},
	date-modified = {2021-09-22 03:53:56 +1000},
	howpublished = {Website},
	title = {My position on "Power Poses''},
	year = 2015}

@article{Carney2010-jp,
	abstract = {Humans and other animals express power through open, expansive
              postures, and they express powerlessness through closed,
              contractive postures. But can these postures actually cause
              power? The results of this study confirmed our prediction that
              posing in high-power nonverbal displays (as opposed to low-power
              nonverbal displays) would cause neuroendocrine and behavioral
              changes for both male and female participants: High-power posers
              experienced elevations in testosterone, decreases in cortisol,
              and increased feelings of power and tolerance for risk; low-power
              posers exhibited the opposite pattern. In short, posing in
              displays of power caused advantaged and adaptive psychological,
              physiological, and behavioral changes, and these findings suggest
              that embodiment extends beyond mere thinking and feeling, to
              physiology and subsequent behavioral choices. That a person can,
              by assuming two simple 1-min poses, embody power and instantly
              become more powerful has real-world, actionable implications.},
	author = {Carney, Dana and Cuddy, Amy and Yap, Andy},
	date-added = {2021-09-21 17:43:44 +1000},
	date-modified = {2021-12-28 17:39:23 +1100},
	journal = {Psychol. Sci.},
	language = {en},
	month = oct,
	number = 10,
	pages = {1363--1368},
	title = {Power posing: brief nonverbal displays affect neuroendocrine levels and risk tolerance},
	volume = 21,
	year = 2010}

@article{Simmons2011-tp,
	abstract = {In this article, we accomplish two things. First, we show that
               despite empirical psychologists' nominal endorsement of a low
               rate of false-positive findings ($\leq$ .05), flexibility in
               data collection, analysis, and reporting dramatically increases
               actual false-positive rates. In many cases, a researcher is more
               likely to falsely find evidence that an effect exists than to
               correctly find evidence that it does not. We present computer
               simulations and a pair of actual experiments that demonstrate
               how unacceptably easy it is to accumulate (and report)
               statistically significant evidence for a false hypothesis.
               Second, we suggest a simple, low-cost, and straightforwardly
               effective disclosure-based solution to this problem. The
               solution involves six concrete requirements for authors and four
               guidelines for reviewers, all of which impose a minimal burden
               on the publication process.},
	author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
	date-added = {2021-09-21 17:29:03 +1000},
	date-modified = {2021-09-21 17:29:03 +1000},
	journal = {Psychol. Sci.},
	language = {en},
	month = nov,
	number = 11,
	pages = {1359--1366},
	publisher = {SAGE Publications},
	title = {False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant},
	volume = 22,
	year = 2011}

@article{Amrhein2019-tk,
	author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
	copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	date-added = {2021-09-07 20:41:27 +1000},
	date-modified = {2021-09-07 20:41:27 +1000},
	journal = {Am. Stat.},
	language = {en},
	month = mar,
	number = {sup1},
	pages = {262--270},
	publisher = {Informa UK Limited},
	title = {Inferential statistics as descriptive statistics: There is no replication crisis if we don't expect replication},
	volume = 73,
	year = 2019}

@book{Neumayer2017-hc,
	abstract = {The uncertainty that researchers face in specifying their
               estimation model threatens the validity of their inferences. In
               regression analyses of observational data, the 'true model'
               remains unknown, and researchers face a choice between plausible
               alternative specifications. Robustness testing allows
               researchers to explore the stability of their main estimates to
               plausible variations in model specifications. This highly
               accessible book presents the logic of robustness testing,
               provides an operational definition of robustness that can be
               applied in all quantitative research, and introduces readers to
               diverse types of robustness tests. Focusing on each dimension of
               model uncertainty in separate chapters, the authors provide a
               systematic overview of existing tests and develop many new ones.
               Whether it be uncertainty about the population or sample,
               measurement, the set of explanatory variables and their
               functional form, causal or temporal heterogeneity, or effect
               dynamics or spatial dependence, this book provides guidance and
               offers tests that researchers from across the social sciences
               can employ in their own research.},
	author = {Neumayer, Eric and Pl{\"u}mper, Thomas},
	date-added = {2021-09-07 20:38:49 +1000},
	date-modified = {2021-09-07 20:38:49 +1000},
	language = {en},
	month = aug,
	publisher = {Cambridge University Press},
	title = {Robustness Tests for Quantitative Research},
	year = 2017}

@article{Tong2019-as,
	abstract = {ABSTRACTScientific research of all kinds should be guided by
               statistical thinking: in the design and conduct of the study, in
               the disciplined exploration and enlightened display of the data,
               and to avoid statistical pitfalls in the interpretation of the
               results. However, formal, probability-based statistical
               inference should play no role in most scientific research, which
               is inherently exploratory, requiring flexible methods of
               analysis that inherently risk overfitting. The nature of
               exploratory work is that data are used to help guide model
               choice, and under these circumstances, uncertainty cannot be
               precisely quantified, because of the inevitable model selection
               bias that results. To be valid, statistical inference should be
               restricted to situations where the study design and analysis
               plan are specified prior to data collection. Exploratory data
               analysis provides the flexibility needed for most other
               situations, including statistical methods that are regularized,
               robust, or nonparametric. Of course, no individual statistical
               analysis should be considered sufficient to establish scientific
               validity: research requires many sets of data along many lines
               of evidence, with a watchfulness for systematic error.
               Replicating and predicting findings in new data and new settings
               is a stronger way of validating claims than blessing results
               from an isolated study with statistical inferences.},
	author = {Tong, Christopher},
	date-added = {2021-09-07 20:30:32 +1000},
	date-modified = {2021-09-07 20:30:32 +1000},
	journal = {Am. Stat.},
	month = mar,
	number = {sup1},
	pages = {246--261},
	publisher = {Taylor \& Francis},
	title = {Statistical Inference Enables Bad Science; Statistical Thinking Enables Good Science},
	volume = 73,
	year = 2019}

@article{Gelman2014-wz,
	author = {Gelman, Andrew and Loken, Eric},
	date-added = {2021-09-07 20:19:16 +1000},
	date-modified = {2021-09-07 20:19:16 +1000},
	journal = {Am. Sci.},
	keywords = {Periodical publishing; Science journals},
	pages = {460+},
	title = {The statistical crisis in science: data-dependent analysis--a ``garden of forking paths''--explains why many statistically significant comparisons don't hold up},
	volume = 102,
	year = 2014}

@article{Scheel2021-wa,
	abstract = {Selectively publishing results that support the tested
               hypotheses (?positive? results) distorts the available evidence
               for scientific claims. For the past decade, psychological
               scientists have been increasingly concerned about the degree of
               such distortion in their literature. A new publication format
               has been developed to prevent selective reporting: In Registered
               Reports (RRs), peer review and the decision to publish take
               place before results are known. We compared the results in
               published RRs (N = 71 as of November 2018) with a random sample
               of hypothesis-testing studies from the standard literature (N =
               152) in psychology. Analyzing the first hypothesis of each
               article, we found 96\% positive results in standard reports but
               only 44\% positive results in RRs. We discuss possible
               explanations for this large difference and suggest that a
               plausible factor is the reduction of publication bias and/or
               Type I error inflation in the RR literature.},
	author = {Scheel, Anne M and Schijen, Mitchell R M J and Lakens, Dani{\"e}l},
	date-added = {2021-09-07 20:15:29 +1000},
	date-modified = {2022-01-16 18:11:39 +1100},
	journal = {Advances in Methods and Practices in Psychological Science},
	language = {en},
	month = apr,
	number = 2,
	publisher = {SAGE Publications Inc},
	title = {An Excess of Positive Results: Comparing the Standard Psychology Literature With Registered Reports},
	volume = 4,
	year = 2021}

@incollection{Abbuhl2018-zs,
	abstract = {This chapter traces the history of replication research in the
               field of applied linguistics, culminating in a discussion of
               current views of replication research as a means of evaluating
               the internal and external validity of a study, illuminating
               phenomena of interest, and ultimately, driving both theory and
               pedagogy forward. It provides an overview of different types of
               replication studies (exact, approximate, conceptual) with recent
               examples from the field. Challenges concerning the
               interpretation of replication results, as well as ongoing
               controversies over the replication of qualitative studies, are
               discussed. The author concludes with current recommendations for
               facilitating replication research, including those pertaining to
               reporting and data sharing.},
	address = {London},
	author = {Abbuhl, Rebekha},
	booktitle = {The Palgrave Handbook of Applied Linguistics Research Methodology},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	editor = {Phakiti, Aek and De Costa, Peter and Plonsky, Luke and Starfield, Sue},
	pages = {145--162},
	publisher = {Palgrave Macmillan UK},
	title = {Research Replication},
	year = 2018}

@misc{Arceneaux2018-nj,
	abstract = {Cambridge Core - Journal of Experimental Political Science -
                  Volume 5 -},
	author = {Arceneaux, Kevin and Wilson, Rick and Boudreau, Cheryl and Bush, Sarah and Jerit, Jennifer and Rubenson, Daniel and Sinclair, Betsy and Settle, Jaime and Woon, Jonathan and Zechmeister, Elizabeth and Buntaine, Mark T and Prather, Lauren and Kingsley, David C and Muise, Daniel and Merolla, Jennifer L and Zechmeister, Elizabeth J and Busby, Ethan C and Druckman, James N and Clayton, Amanda and Bonilla, Tabitha and Mo, Cecilia Hyunjung and Zhang, Nan and Enos, Ryan D and Celaya, Christopher and Amira, Karyn and Severson, Alexander W and Linos, Katerina and Twist, Kimberly and Ihme, Toni Alexander and Tausendpfund, Markus and Trump, Kris-Stella and White, Ariel and Strezhnev, Anton and Lucas, Christopher and Kruszewska, Dominika and Huff, Connor and Porter, Ethan and Wood, Thomas J and Kirby, David and Andersen, David J and Lau, Richard R and Morin-Chass{\'e}, Alexandre},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	howpublished = {\url{https://www.cambridge.org/core/journals/journal-of-experimental-political-science/volume/DAE99D693937B819850C6FF857EB24A9?pageNum=1}},
	note = {Accessed: 2021-7-15},
	publisher = {cambridge.org},
	title = {Journal of Experimental Political Science: Volume 5 -},
	year = 2018}

@article{Arceneaux2018-yg,
	abstract = {In the interest of promoting open and reproducible science, the
               Journal of Experimental Political Science editorial team will
               pilot the pre-acceptance of preregistered reports. We note that
               the launch of this new submission option is a complement to, and
               does not replace, the option to submit other types of
               manuscripts. JEPS remains open to receiving and reviewing high
               quality manuscripts regardless of whether they are based on
               preregistered studies.},
	address = {Washington, United Kingdom, Washington},
	author = {Arceneaux, Kevin and Wilson, Rick and Boudreau, Cheryl and Bush, Sarah and Jerit, Jennifer and Rubenson, Daniel and Sinclair, Betsy and Settle, Jaime and Woon, Jonathan and Zechmeister, Elizabeth},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Journal of Experimental Political Science; Washington},
	language = {en},
	number = 3,
	pages = {165--166},
	publisher = {Cambridge University Press},
	title = {The Value of Preregistration for Open and Credible Science: An Initiative from the Journal of Experimental Political Science},
	volume = 5,
	year = 2018}

@article{Auspurg2021-dy,
	abstract = {In 2018, Silberzahn, Uhlmann, Nosek, and colleagues published an
               article in which 29 teams analyzed the same research question
               with the same data: Are soccer referees more likely to give red
               cards to players with dark skin tone than light skin tone? The
               results obtained by the teams differed extensively. Many
               concluded from this widely noted exercise that the social
               sciences are not rigorous enough to provide definitive answers.
               In this article, we investigate why results diverged so much. We
               argue that the main reason was an unclear research question:
               Teams differed in their interpretation of the research question
               and therefore used diverse research designs and model
               specifications. We show by reanalyzing the data that with a
               clear research question, a precise definition of the parameter
               of interest, and theory-guided causal reasoning, results vary
               only within a narrow range. The broad conclusion of our
               reanalysis is that social science research needs to be more
               precise in its ``estimands'' to become credible.},
	author = {Auspurg, Katrin and Br{\"u}derl, Josef},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Socius},
	keywords = {trust\_us},
	language = {en},
	month = jan,
	pages = {237802312110244},
	publisher = {SAGE Publications},
	title = {Has the credibility of the social sciences been credibly destroyed? Reanalyzing the ``many analysts, one data set'' project},
	volume = 7,
	year = 2021}

@article{Barba2018-ak,
	abstract = {Reproducible research---by its many names---has come to be
                   regarded as a key concern across disciplines and stakeholder
                   groups. Funding agencies and journals, professional
                   societies and even mass media are paying attention, often
                   focusing on the so-called ``crisis'' of reproducibility. One
                   big problem keeps coming up among those seeking to tackle
                   the issue: different groups are using terminologies in utter
                   contradiction with each other. Looking at a broad sample of
                   publications in different fields, we can classify their
                   terminology via decision tree: they either, A---make no
                   distinction between the words reproduce and replicate, or
                   B---use them distinctly. If B, then they are commonly
                   divided in two camps. In a spectrum of concerns that starts
                   at a minimum standard of ``same data+same methods=same
                   results,'' to ``new data and/or new methods in an
                   independent study=same findings,'' group 1 calls the minimum
                   standard reproduce, while group 2 calls it replicate. This
                   direct swap of the two terms aggravates an already weighty
                   issue. By attempting to inventory the terminologies across
                   disciplines, I hope that some patterns will emerge to help
                   us resolve the contradictions.},
	archiveprefix = {arXiv},
	author = {Barba, Lorena A},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	eprint = {1802.03311},
	month = feb,
	primaryclass = {cs.DL},
	title = {Terminologies for Reproducible Research},
	year = 2018}

@misc{Bartos_2020-ar,
	author = {Barto{\v s}, Franti{\v s}ek and Schimmack, Ulrich},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	howpublished = {\url{https://psyarxiv.com/urgtn/download?format=pdf}},
	keywords = {trust\_us},
	note = {Accessed: 2021-9-4},
	title = {Z-curve 2.0: Estimating replication rates and discovery rates},
	year = {2020}}

@incollection{Bausell_undated-kl,
	abstract = {{\ldots} most powerful and relatively painless strategies for improving
               the reproducibility of scientific {\ldots} approximately two decades
               ago by federal legislation requiring the preregistration of all
               {\ldots} nonmedical disciplines although the results are disappointing
               when preregistered protocols are {\ldots}},
	author = {Bausell, R Barker},
	booktitle = {The Problem with Science},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	pages = {222--260},
	publisher = {Oxford University Press},
	title = {Preregistration, Data Sharing, and Other Salutary Behaviors}}

@article{Berinsky2021-rk,
	abstract = {One of the strongest findings across the sciences is that
               publication bias occurs. Of particular note is a ``file drawer
               bias'' where statistically significant results are privileged
               over nonsignificant results. Recognition of this bias, along
               with increased calls for ``open science,'' has led to an
               emphasis on replication studies. Yet, few have explored
               publication bias and its consequences in replication studies. We
               offer a model of the publication process involving an initial
               study and a replication. We use the model to describe three
               types of publication biases: (1) file drawer bias, (2) a
               ``repeat study'' bias against the publication of replication
               studies, and (3) a ``gotcha bias'' where replication results
               that run contrary to a prior study are more likely to be
               published. We estimate the model's parameters with a vignette
               experiment conducted with political science professors teaching
               at Ph.D. granting institutions in the United States. We find
               evidence of all three types of bias, although those explicitly
               involving replication studies are notably smaller. This bodes
               well for the replication movement. That said, the aggregation of
               all of the biases increases the number of false positives in a
               literature. We conclude by discussing a path for future work on
               publication biases.},
	author = {Berinsky, Adam J and Druckman, James N and Yamamoto, Teppei},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Polit. Anal.},
	keywords = {publication bias; replication studies; file drawer bias; open science; conjoint experiment},
	month = jul,
	number = 3,
	pages = {370--384},
	publisher = {Cambridge University Press},
	title = {Publication Biases in Replication Studies},
	volume = 29,
	year = 2021}

@article{Brady2016-qd,
	abstract = {Experimental approaches to political science research have
               become increasingly prominent in the discipline. Experimental
               research is regularly featured in some of the discipline's top
               journals, and indeed in 2014 a new Journal of Experimental
               Political Science was created, published by Cambridge University
               Press. At the same time, there are disagreements among political
               scientists about the limits of experimental research, the
               ethical challenges associated with this research, and the
               general model of social scientific inquiry underlying much
               experimental research. Field Experiments and Their Critics:
               Essays on the Uses and Abuses of Experimentation in the Social
               Sciences, edited by Dawn Langan Teele (Yale University Press
               2015), brings together many interesting perspectives on these
               issues. And so we have invited a number of political scientists
               to comment on the book, the issues it raises, and the more
               general question of ``the uses and abuses of experimentation in
               the social sciences.''},
	author = {Brady, Henry E},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Perspect. politics},
	language = {en},
	month = dec,
	number = 4,
	pages = {1130--1131},
	publisher = {Cambridge University Press (CUP)},
	title = {A discussion of Dawn Langan Teele's field Experiments and Their Critics: Essays on the uses and abuses of experimentation in the Social Sciences},
	volume = 14,
	year = 2016}

@article{Brodeur2020-ld,
	abstract = {Methods Matter: p-Hacking and Publication Bias in Causal Analysis
              in Economics by Abel Brodeur, Nikolai Cook and Anthony Heyes.
              Published in volume 110, issue 11, pages 3634-60 of American
              Economic Review, November 2020, Abstract: The credibility
              revolution in economics has promoted causal identific...},
	author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Am. Econ. Rev.},
	keywords = {trust\_us},
	month = nov,
	number = 11,
	pages = {3634--3660},
	title = {Methods Matter: p-Hacking and Publication Bias in Causal Analysis in Economics},
	volume = 110,
	year = 2020}

@inproceedings{Brown2016-tt,
	abstract = {Replication is one of the main principles of the
                     scientific method. In the physical sciences, new knowledge
                     is often not considered valid until the original study has
                     been replicated in other labs and the original results are
                     not refuted Replication will either improve confidence in
                     our research findings or identify important boundary
                     conditions. Replications also enhance various scientific
                     processes and offer methodical and educational
                     improvements. The purpose of this panel is twofold. First,
                     to explore the opportunities for scientific development
                     that replication research enables by reflecting on the
                     experiences of encouraging, doing, and publishing
                     replication studies. Second, to explore the various
                     challenges that replication research raises about its
                     value to individual scholars as well as to our collective
                     understanding of phenomena within the information systems
                     field.},
	address = {IRL},
	author = {Brown, Sue and Dennis, Alan R and Binny, Samuel and Tan, Barney and Valacich, Joseph and Whitley, Edgar A},
	conference = {Thirty Seventh International Conference on Information Systems},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {replication, research methods/methodology, theory building},
	language = {en},
	month = dec,
	pages = {1--5},
	publisher = {eprints.lse.ac.uk},
	title = {Replication research: opportunities, experiences and challenges},
	year = 2016}

@article{Carney_undated-sy,
	author = {Carney, Regarding:},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {trust\_us},
	title = {My position on ``Power Poses''}}

@article{Chin2021-px,
	abstract = {Questionable research practices (QRPs) lead to incorrect research
              results and contribute to irreproducibility in science.
              Researchers and institutions have proposed open science practices
              (OSPs) to improve the detectability of QRPs and the credibility
              of science. We examine the prevalence of QRPs and OSPs in
              criminology, and researchers' opinions of those practices.},
	author = {Chin, Jason M and Pickett, Justin T and Vazire, Simine and Holcombe, Alex O},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {J. Quant. Criminol.},
	month = aug,
	title = {Questionable Research Practices and Open Science in Quantitative Criminology},
	year = 2021}

@article{Christensen2020-fc,
	abstract = {Author(s): Christensen, Garret; Wang, Zenan; Levy Paluck,
               Elizabeth; Swanson, Nicholas; Birke, David; Miguel, Edward;
               Littman, Rebecca | Abstract: Has there been meaningful movement
               toward open sci-ence practices within the social sciences in
               recent years? Discussions about changes in practices such as
               posting data and pre-registering analyses have been marked by
               controversy---including controversy over the extent to which
               change has taken place. This study, based on the State of Social
               Science (3S) Survey, provides the first com-prehensive
               assessment of awareness of, attitudes towards, perceived norms
               regarding, and adoption of open science practices within a
               broadly representative sample of scholars from four major social
               science disciplines: economics, political science, psychology,
               and so-ciology. We observe a steep increase in adoption: as of
               2017, over 80\% of scholars had used at least one such practice,
               rising from one quarter a decade earlier. Attitudes toward
               research transpar-ency are on average similar between older and
               younger scholars, but the pace of change diers by field and
               methodology. According with theories of normal science and
               scientific change, the timing of increases in adoption coincides
               with technological innovations and institutional policies.
               Patterns are consistent with most scholars underestimating the
               trend toward open science in their discipline.},
	author = {Christensen, Garret and Wang, Zenan and Levy Paluck, Elizabeth and Swanson, Nicholas and Birke, David and Miguel, Edward and Littman, Rebecca},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {Social and Behavioral Sciences},
	month = jan,
	publisher = {escholarship.org},
	title = {Open Science Practices are on the Rise: The State of Social Science (3S) Survey},
	year = 2020}

@unpublished{Christian2018-ox,
	abstract = {In response to widespread concerns about the integrity of
              research published in scholarly journals, several initiatives
              have emerged that are promoting research transparency through
              access to data underlying published scientific findings. Journal
              editors, in particular, have made a commitment to research
              transparency by issuing data policies that require authors to
              submit their data, code, and documentation to data repositories
              to allow for public access to the data. In the case of the
              American Journal of Political Science (AJPS) Data Replication
              Policy, the data also must undergo an independent verification
              process in which materials are reviewed for quality as a
              condition of final manuscript publication and acceptance. Aware
              of the specialized expertise of the data archives, AJPS called
              upon the Odum Institute Data Archive to provide a data review
              service that performs data curation and verification of
              replication datasets. This article presents a case study of the
              collaboration between AJPS and the Odum Institute Data Archive to
              develop a workflow that bridges manuscript publication and data
              review processes. The case study describes the challenges and the
              successes of the workflow integration, and offers lessons learned
              that may be applied by other data archives that are considering
              expanding their services to include data curation and
              verification services to support reproducible research.},
	author = {Christian, Thu-Mai L and Lafferty-Hess, Sophia and Jacoby, William G and Carsey, Thomas M},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {data curation; data policy; data verification; replication; reproducibility},
	month = apr,
	title = {Operationalizing the Replication Standard: A Case Study of the Data Curation and Verification Workflow for Scholarly Journals},
	year = 2018}

@article{Crosas2015-xh,
	abstract = {The vast majority of social science research uses small
               (megabyte- or gigabyte-scale) datasets. These fixed-scale
               datasets are commonly downloaded to the researcher?s computer
               where the analysis is performed. The data can be shared,
               archived, and cited with well-established technologies, such as
               the Dataverse Project, to support the published results. The
               trend toward big data?including large-scale streaming data?is
               starting to transform research and has the potential to impact
               policymaking as well as our understanding of the social,
               economic, and political problems that affect human societies.
               However, big data research poses new challenges to the execution
               of the analysis, archiving and reuse of the data, and
               reproduction of the results. Downloading these datasets to a
               researcher?s computer is impractical, leading to analyses taking
               place in the cloud, and requiring unusual expertise,
               collaboration, and tool development. The increased amount of
               information in these large datasets is an advantage, but at the
               same time it poses an increased risk of revealing personally
               identifiable sensitive information. In this article, we discuss
               solutions to these new challenges so that the social sciences
               can realize the potential of big data.},
	author = {Crosas, Merc{\`e} and King, Gary and Honaker, James and Sweeney, Latanya},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Ann. Am. Acad. Pol. Soc. Sci.},
	month = may,
	number = 1,
	pages = {260--273},
	publisher = {SAGE Publications Inc},
	title = {Automating Open Science for Big Data},
	volume = 659,
	year = 2015}

@article{Culina2020-xr,
	abstract = {Access to analytical code is essential for transparent and
               reproducible research. We review the state of code availability
               in ecology using a random sample of 346 nonmolecular articles
               published between 2015 and 2019 under mandatory or encouraged
               code-sharing policies. Our results call for urgent action to
               increase code availability: only 27\% of eligible articles were
               accompanied by code. In contrast, data were available for 79\%
               of eligible articles, highlighting that code availability is an
               important limiting factor for computational reproducibility in
               ecology. Although the percentage of ecological journals with
               mandatory or encouraged code-sharing policies has increased
               considerably, from 15\% in 2015 to 75\% in 2020, our results
               show that code-sharing policies are not adhered to by most
               authors. We hope these results will encourage journals,
               institutions, funding agencies, and researchers to address this
               alarming situation.},
	author = {Culina, Antica and van den Berg, Ilona and Evans, Simon and S{\'a}nchez-T{\'o}jar, Alfredo},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PLoS Biol.},
	language = {en},
	month = jul,
	number = 7,
	pages = {e3000763},
	publisher = {Public Library of Science (PLoS)},
	title = {Low availability of code in ecology: A call for urgent action},
	volume = 18,
	year = 2020}

@article{Dafoe2014-ws,
	abstract = {In April 2013, a controversy arose when a working paper
               (Herndon, Ash, and Pollin 2013) claimed to show serious errors
               in a highly cited and influential economics paper by Carmen
               Reinhart and Kenneth Rogoff (2010). The Reinhart and Rogoff
               paper had come to serve as authoritative evidence in elite
               conversations (Krugman 2013) that high levels of debt,
               especially above the ``90 percent [debt/GDP] threshold''
               (Reinhart and Rogoff 2010, 577), posed a risk to economic
               growth. Much of the coverage of this controversy focused on an
               error that was a ``perfect made-for-TV mistake'' (Stevenson and
               Wolfers 2013) involving a simple error in the formula used in
               their Excel calculations. The real story here, however, is that
               it took three years for this error and other issues to be
               discovered because replication files were not publicly
               available, nor were they provided to scholars when asked. If
               professional norms or the American Economic Review had required
               that authors publish replication files, this debate would be
               advanced by three years and discussions about austerity policies
               would have been based on a more clear-sighted appraisal of the
               evidence.},
	author = {Dafoe, Allan},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = jan,
	number = 1,
	pages = {60--66},
	publisher = {Cambridge University Press},
	title = {Science Deserves Better: The Imperative to Share Complete Replication Files},
	volume = 47,
	year = 2014}

@techreport{DeHaven2020-ls,
	abstract = {Final Technical Report for the Center for Open Sciences
                 efforts on the Next Generation Social Sciences Program. COS
                 assisted performing teams with making data open and workflows
                 reproducible, evaluated the reproducibility of performers
                 efforts, and facilitated the preregistration of performer
                 experiments and analyses.},
	author = {DeHaven, Alexander and Errington, Timothy and Brooks, Ronald and Nosek, Brian and {Center for Open Science, Inc.}},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	institution = {Center for Open Science, Inc.},
	keywords = {information science, data science, political science, statistics, psychology, social psychology, social sciences, public policy, artificial intelligence, cognitive science, computer programming, data analysis, economics, experimental data, experimental design, literature surveys, sociology, training, high density, low density},
	month = nov,
	publisher = {apps.dtic.mil},
	title = {A comprehensive research content and workflow pipeline to increase openness, reproducibility, and prediction in social science research},
	year = 2020}

@article{DeSoto2017-cb,
	abstract = {Researchers agree that replicability and reproducibility are key
               aspects of science. A collection of Data Descriptors published
               in Scientific Data presents data obtained in the process of
               attempting to replicate previously published research. These new
               replication data describe published and unpublished projects.
               The different papers in this collection highlight the many ways
               that scientific replications can be conducted, and they reveal
               the benefits and challenges of crucial replication research. The
               organizers of this collection encourage scientists to reuse the
               data contained in the collection for their own work, and also
               believe that these replication examples can serve as educational
               resources for students, early-career researchers, and
               experienced scientists alike who are interested in learning more
               about the process of replication.},
	author = {DeSoto, K Andrew and Schweinsberg, Martin},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Sci Data},
	language = {en},
	month = mar,
	pages = {170028},
	publisher = {nature.com},
	title = {Replication data collection highlights value in diversity of replication attempts},
	volume = 4,
	year = 2017}

@article{Duvendack2017-zo,
	abstract = {This paper discusses recent trends in the use of replications in
               economics. We include the results of recent replication studies
               that have attempted to identify replication rates within the
               discipline. These studies generally find that replication rates
               are relatively low. We then consider obstacles to undertaking
               replication studies and highlight replication initiatives in
               psychology and political science, behind which economics appears
               to lag.},
	author = {Duvendack, Maren and Palmer-Jones, Richard and Reed, W Robert},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Am. Econ. Rev.},
	language = {en},
	month = may,
	number = 5,
	pages = {46--51},
	publisher = {American Economic Association},
	title = {What is meant by ``replication'' and why does it encounter resistance in economics?},
	volume = 107,
	year = 2017}

@article{Elman2018-io,
	abstract = {Political scientists use diverse methods to study important
               topics. The findings they reach and conclusions they draw can
               have significant social implications and are sometimes
               controversial. As a result, audiences can be skeptical about the
               rigor and relevance of the knowledge claims that political
               scientists produce. For these reasons, being a political
               scientist means facing myriad questions about how we know what
               we claim to know. Transparency can help political scientists
               address these questions. An emerging literature and set of
               practices suggest that sharing more data and providing more
               information about our analytic and interpretive choices can help
               others understand the rigor and relevance of our claims. At the
               same time, increasing transparency can be costly and has been
               contentious. This review describes opportunities created by, and
               difficulties posed by, attempts to increase transparency. We
               conclude that, despite the challenges, consensus about the value
               and practice of transparency is emerging within and across
               political science's diverse and dynamic research communities.},
	author = {Elman, Colin and Kapiszewski, Diana and Lupia, Arthur},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Annu. Rev. Polit. Sci.},
	month = may,
	number = 1,
	pages = {29--47},
	publisher = {Annual Reviews},
	title = {Transparent Social Inquiry: Implications for Political Science},
	volume = 21,
	year = 2018}

@article{Engzell2021-hn,
	author = {Engzell, Per and Rohrer, Julia M},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	keywords = {trust\_us},
	language = {en},
	month = apr,
	number = 2,
	pages = {297--300},
	publisher = {Cambridge University Press (CUP)},
	title = {Improving social science: Lessons from the open science movement},
	volume = 54,
	year = 2021}

@article{Eubank2016-hq,
	abstract = {To allow researchers to investigate not only whether a paper's
                 methods are theoretically sound but also whether they have
                 been properly implemented and are robust to alternative
                 specifications, it is necessary that published papers be
                 accompanied by their underlying data and code. This article
                 describes experiences and lessons learned at the Quarterly
                 Journal of Political Science since it began requiring authors
                 to provide this type of replication code in 2005. It finds
                 that of the 24 empirical papers subjected to in-house
                 replication review since September 2012, only four packages
                 did not require any modifications. Most troubling, 14 packages
                 (58\%) had results in the paper that differed from those
                 generated by the author's own code. Based on these
                 experiences, this article presents a set of guidelines for
                 authors and journals for improving the reliability and
                 usability of replication packages.},
	author = {Eubank, Nicholas},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	keywords = {trust\_us},
	month = apr,
	number = 2,
	original_id = {d91030f6-343f-0465-88a8-62bec0a37fbc},
	pages = {273--276},
	publisher = {Cambridge University Press},
	title = {Lessons from a Decade of Replications at the Quarterly Journal of Political Science},
	volume = 49,
	year = 2016}

@article{Figueiredo2014-ri,
	abstract = {{\ldots} Comparatively, the Dataverse Network, organized by The
               Institute for Quantitative Social Science of Harvard University,
               has the largest data sharing archive in Social Sciences . The
               system is open and very user friendly {\ldots}},
	author = {Figueiredo, Dalson and Rocha, Enivaldo and Paranhos, Ranulfo and Alexandre, Jos{\'e}},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	publisher = {academia.edu},
	title = {Replication in Political Science: the case of {SIGOBR}},
	year = 2014}

@article{Figueiredo2019-gx,
	abstract = {{\ldots} Thus, reproducible mandatory policies on reproducibility can
               reduce the likelihood of scientific frauds. Besides preventing
               intentional deception, reproducible research also avoids honest
               mistakes {\ldots} PS: Political Science and Politics . Vol. 28, N 03,
               pp. 444-452 {\ldots}},
	author = {Figueiredo, Dalson and Lins, Rodrigo and Domingos, Amanda and Janz, Nicole and Silva, Lucas},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Brazilian Political Science Review},
	publisher = {SciELO Brasil},
	title = {Seven reasons why: a user's guide to transparency and reproducibility},
	volume = 13,
	year = 2019}

@article{Franco2014-nw,
	abstract = {[We studied publication bias in the social sciences by analyzing
               a known population of conducted studies---221 in total---in
               which there is a full accounting of what is published and
               unpublished. We leveraged Time-sharing Experiments in the Social
               Sciences (TESS), a National Science Foundation--sponsored
               program in which researchers propose survey-based experiments to
               be run on representative samples of American adults. Because
               TESS proposals undergo rigorous peer review, the studies in the
               sample all exceed a substantial quality threshold. Strong
               results are 40 percentage points more likely to be published
               than are null results and 60 percentage points more likely to be
               written up. We provide direct evidence of publication bias and
               identify the stage of research production at which publication
               bias occurs: Authors do not write up and submit null findings.]},
	author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Science},
	keywords = {trust\_us},
	number = 6203,
	pages = {1502--1505},
	publisher = {American Association for the Advancement of Science},
	title = {Unlocking the file drawer},
	volume = 345,
	year = 2014}

@article{Gherghina2013-hf,
	abstract = {A characteristic of recent decades of scholarly work in the
              social sciences has been the increased amounts of empirical
              research. Access and availability of data are prerequisites for
              further research, replication work, and scientific development.
              As international peer-reviewed journals have gradually become the
              central forum for research debate, moves towards data sharing are
              dependent upon the policies of journals regarding data
              availability. This article examines contemporary data
              availability policies in political science and investigates the
              extent to which journals adopt such policies and their content.
              It also identifies a few factors associated with the existence of
              such policies.},
	author = {Gherghina, Sergiu and Katsanidou, Alexia},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {European Political Science},
	keywords = {trust\_us},
	month = sep,
	number = 3,
	pages = {333--349},
	title = {Data Availability in Political Science Journals},
	volume = 12,
	year = 2013}

@misc{Gill_undated-ds,
	abstract = {{\ldots} I am the last person to have ``natural science envy'' as I
                  also pay attention to the {\ldots} An interesting experiment would
                  be to give two political scientists the exact same dataset
                  and a single {\ldots} This leads to a final point: it is vital to
                  keep open , positive, and professional communication with {\ldots}},
	author = {Gill, J},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	howpublished = {\url{https://www.researchgate.net/profile/Jeff-Gill-2/publication/330029282_Adventures_in_Replication_An_Introduction_to_the_Forum/links/5f5637f6a6fdcc9879d32e5a/Adventures-in-Replication-An-Introduction-to-the-Forum.pdf}},
	note = {Accessed: 2021-7-15},
	title = {[No title]}}

@article{Gill2019-qx,
	abstract = {Adventures in Replication: An Introduction to the Forum - Volume
               27 Issue 1},
	author = {Gill, Jeff},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Polit. Anal.},
	month = jan,
	number = 1,
	pages = {98--100},
	publisher = {Cambridge University Press},
	title = {Adventures in Replication: An Introduction to the Forum},
	volume = 27,
	year = 2019}

@article{Golden1995-tj,
	abstract = {In ``Replication, Replication,'' Gary King convincingly argues
               that publications by political scientists should adhere to what
               he calls a replication standard. Although King's article
               explicitly embraces qualitative as well as quantitative
               research, the policy statement that he suggests editors and
               reviewers of books and journals endorse exclusively imposes
               standards on studies based on quantitative research. Qualitative
               work is treated in a simple sentence and one that enforces no
               standards: ``Authors of works relying upon qualitative data are
               encouraged (but not required) to submit a comparable footnote
               that would facilitate replication where feasible.''},
	author = {Golden, Miriam A},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = sep,
	number = 3,
	pages = {481--483},
	publisher = {Cambridge University Press},
	title = {Replication and {Non-Quantitative} Research},
	volume = 28,
	year = 1995}

@article{Grossman2020-yv,
	abstract = {Replicability in political science is on the rise, as
                 disciplinary journals have been placing a growing emphasis on
                 data access and research transparency (DA--RT) practices and
                 policies. As a result, nearly every article that is published
                 today in leading political science journals offers an online
                 appendix that includes data, code, and methodological
                 explanations necessary for replication. While these
                 developments are laudable, many appendices still do not enable
                 satisfactory replication because they are inaccessible,
                 compartmentalized, and difficult to understand. In this
                 article and in its accompanying online appendix, we
                 demonstrate this problem and make the case for more accessible
                 and comprehensive appendices whose contribution can fulfill
                 and go beyond mere replicability. We propose several ways in
                 which authors and journals can produce better appendices,
                 namely, by making appendices more intuitive, integrated, and
                 standardized, and by choosing an adequate online platform on
                 which to create and host the appendix.},
	author = {Grossman, Jonathan and Pedahzur, Ami},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Perspectives on Politics},
	original_id = {99ebad5c-453b-0edc-8385-af0a98e94209},
	pages = {1--6},
	publisher = {Cambridge University Press},
	title = {Can We Do Better? Replication and Online Appendices in Political Science},
	year = 2020}

@article{Hardwicke2021-fp,
	abstract = {Psychologists are navigating an unprecedented period of
               introspection about the credibility and utility of their
               discipline. Reform initiatives emphasize the benefits of
               transparency and reproducibility-related research practices;
               however, adoption across the psychology literature is unknown.
               Estimating the prevalence of such practices will help to gauge
               the collective impact of reform initiatives, track progress over
               time, and calibrate future efforts. To this end, we manually
               examined a random sample of 250 psychology articles published
               between 2014 and 2017. Over half of the articles were publicly
               available (154/237, 65\%, 95\% confidence interval [CI] = [59\%,
               71\%]); however, sharing of research materials (26/183; 14\%,
               95\% CI = [10\%, 19\%]), study protocols (0/188; 0\%, 95\% CI =
               [0\%, 1\%]), raw data (4/188; 2\%, 95\% CI = [1\%, 4\%]), and
               analysis scripts (1/188; 1\%, 95\% CI = [0\%, 1\%]) was rare.
               Preregistration was also uncommon (5/188; 3\%, 95\% CI = [1\%,
               5\%]). Many articles included a funding disclosure statement
               (142/228; 62\%, 95\% CI = [56\%, 69\%]), but
               conflict-of-interest statements were less common (88/228; 39\%,
               95\% CI = [32\%, 45\%]). Replication studies were rare (10/188;
               5\%, 95\% CI = [3\%, 8\%]), and few studies were included in
               systematic reviews (21/183; 11\%, 95\% CI = [8\%, 16\%]) or
               meta-analyses (12/183; 7\%, 95\% CI = [4\%, 10\%]). Overall, the
               results suggest that transparency and reproducibility-related
               research practices were far from routine. These findings
               establish baseline prevalence estimates against which future
               progress toward increasing the credibility and utility of
               psychology research can be compared.},
	author = {Hardwicke, Tom E and Thibault, Robert T and Kosie, Jessica E and Wallach, Joshua D and Kidwell, Mallory C and Ioannidis, John P A},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2022-01-16 18:13:33 +1100},
	journal = {Perspect. Psychol. Sci.},
	keywords = {meta-research; open science; psychology; reproducibility; transparency},
	language = {en},
	month = mar,
	publisher = {SAGE Publications},
	title = {Estimating the prevalence of transparency and reproducibility-related research practices in psychology (2014-2017)},
	year = 2021}

@misc{Hartgerink_undated-xn,
	author = {Hartgerink, Chris Hubertus Joseph and Voelkel, Jan G and Wicherts, Jelte M and van Assen, Marcel A L M},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {trust\_us},
	title = {Detection of data fabrication using statistical tools}}

@article{Head2015-vw,
	abstract = {A focus on novel, confirmatory, and statistically significant
              results leads to substantial bias in the scientific literature.
              One type of bias, known as ``p-hacking,'' occurs when researchers
              collect or select data or statistical analyses until
              nonsignificant results become significant. Here, we use
              text-mining to demonstrate that p-hacking is widespread
              throughout science. We then illustrate how one can test for
              p-hacking when performing a meta-analysis and show that, while
              p-hacking is probably common, its effect seems to be weak
              relative to the real effect sizes being measured. This result
              suggests that p-hacking probably does not drastically alter
              scientific consensuses drawn from meta-analyses.},
	author = {Head, Megan L and Holman, Luke and Lanfear, Rob and Kahn, Andrew T and Jennions, Michael D},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PLoS Biol.},
	keywords = {trust\_us},
	language = {en},
	month = mar,
	number = 3,
	pages = {e1002106},
	title = {The extent and consequences of p-hacking in science},
	volume = 13,
	year = 2015}

@article{Hilgard2021-bx,
	abstract = {Effect sizes in social psychology are generally not large and
               are limited by error variance in manipulation and measurement.
               Effect sizes exceeding these limits are implausible and should
               be viewed with skepticism. Maximal positive controls,
               experimental conditions that should show an obvious and
               predictable effect, can provide estimates of the upper limits of
               plausible effect sizes on a measure. In this work, maximal
               positive controls are conducted for three measures of aggressive
               cognition, and the effect sizes obtained are compared to studies
               found through systematic review. Questions are raised regarding
               the plausibility of certain reports with effect sizes comparable
               to, or in excess of, the effect sizes found in maximal positive
               controls. Maximal positive controls may provide a means to
               identify implausible study results at lower cost than direct
               replication.},
	author = {Hilgard, Joseph},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {J. Exp. Soc. Psychol.},
	keywords = {Violent video games; Aggression; Aggressive thought; Positive controls; Scientific self-correction;trust\_us},
	month = mar,
	pages = {104082},
	publisher = {Elsevier},
	title = {Maximal positive controls: A method for estimating the largest plausible effect size},
	volume = 93,
	year = 2021}

@article{Hoelter2016-ny,
	abstract = {{\ldots} organizations such as the Berkeley Initiative for Transparency
               in the Social Sciences (BITSS) and the Center for Open Science
               are offering mechanisms through which individuals can register
               pre {\ldots} plans before beginning analysis offers a large step
               forward for the social science {\ldots}},
	author = {Hoelter, Lynette and Pienta, Amy and Lyle, Jared},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {The SAGE handbook of survey methodology},
	pages = {651--661},
	publisher = {methods.sagepub.com},
	title = {Data preservation, secondary analysis, and replication: Learning from existing data},
	year = 2016}

@article{Hoffler2017-oo,
	abstract = {Replication and Economics Journal Policies by Jan H.
                 H{\"o}ffler. Published in volume 107, issue 5, pages 52-55 of
                 American Economic Review, May 2017, Abstract: Economics
                 journals with reproducibility policies are cited more often
                 than others. For the minority of journals with a mandatory and
                 enforced...},
	author = {H{\"o}ffler, Jan H},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Am. Econ. Rev.},
	keywords = {trust\_us},
	month = may,
	number = 5,
	original_id = {9bba9d02-bf21-0ce7-a1e0-5a4d40bdc1db},
	pages = {52--55},
	publisher = {aeaweb.org},
	title = {Replication and Economics Journal Policies},
	volume = 107,
	year = 2017}

@article{Huntington-Klein2021-tt,
	abstract = {Abstract Researchers make hundreds of decisions about data
               collection, preparation, and analysis in their research. We use
               a many-analysts approach to measure the extent and impact of
               these decisions. Two published causal empirical results are
               replicated by seven replicators each. We find large differences
               in data preparation and analysis decisions, many of which would
               not likely be reported in a publication. No two replicators
               reported the same sample size. Statistical significance varied
               across replications, and for one of the studies the effect's
               sign varied as well. The standard deviation of estimates across
               replications was 3?4 times the mean reported standard error.},
	author = {Huntington-Klein, Nick and Arenas, Andreu and Beam, Emily and Bertoni, Marco and Bloem, Jeffrey R and Burli, Pralhad and Chen, Naibin and Grieco, Paul and Ekpe, Godwin and Pugatch, Todd and Saavedra, Martin and Stopnitzky, Yaniv},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Econ. Inq.},
	language = {en},
	month = jul,
	number = 3,
	pages = {944--960},
	publisher = {Wiley},
	title = {The influence of hidden researcher decisions in applied microeconomics},
	volume = 59,
	year = 2021}

@article{Ishiyama2014-fb,
	abstract = {Recently, the importance of research transparency via
               replication studies has been greatly discussed in most of the
               social sciences, political science included. Indeed, as
               Gherghina and Katsanidou (2013) and Freese (2007) note, to some
               extent, the discussion has been prompted by the tremendous
               changes in publishing in the past decade or so. With the
               enormous expansion in data availability and instant publication
               made possible by the Internet, there now are many opportunities
               to verify the findings presented in the discipline's major
               journals. ``Replication, replication'' has not only become the
               mantra for political science, but for economics, psychology, and
               quantitative sociology as well. These developments opened a
               debate on how to best ``guard the high standards or research
               practice and allow for the maximum use of current knowledge for
               the further development of science'' (Gherghina and Katsanidou
               2013, 1; for similar sentiments see King 1995).},
	author = {Ishiyama, John},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = jan,
	number = 1,
	pages = {78--83},
	publisher = {Cambridge University Press},
	title = {Replication, Research Transparency, and Journal Publications: Individualism, Community Models, and the Future of Replication Studies},
	volume = 47,
	year = 2014}

@article{Janz2016-bt,
	abstract = {Reproducibility is held to be the gold standard for scientific
               research. The credibility of published work depends on being
               able to replicate the results. Howev},
	author = {Janz, Nicole},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Int Stud Perspect},
	language = {en},
	month = feb,
	number = 4,
	pages = {392--407},
	publisher = {Oxford Academic},
	title = {Bringing the Gold Standard into the Classroom: Replication in University Teaching},
	volume = 17,
	year = 2016}

@article{Janz2021-uf,
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000943/resource/name/firstPage-S1049096520000943a.jpg},
	author = {Janz, Nicole and Freese, Jeremy},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = apr,
	number = 2,
	pages = {305--308},
	publisher = {Cambridge University Press},
	title = {Replicate Others as You Would Like to Be Replicated Yourself},
	volume = 54,
	year = 2021}

@book{Jenkins-Smith2017-je,
	abstract = {{\ldots} The first part of the book introduces the scientific method,
               then covers research design, measurement, descriptive {\ldots} The book
               fully embraces the open access and open source philosophies {\ldots} We
               encourage students to download the data, replicate the examples,
               and explore further {\ldots}},
	author = {Jenkins-Smith, Hank C and Ripberger, Joseph T and Copeland, Gary and Nowlin, Matthew C and Hughes, Tyler and Fister, Aaron L and Wehde, Wesley},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	publisher = {shareok.org},
	title = {Quantitative research methods for political science, public policy and public administration (With Applications in R)},
	year = 2017}

@article{John2012-vj,
	abstract = {Cases of clear scientific misconduct have received significant
              media attention recently, but less flagrantly questionable
              research practices may be more prevalent and, ultimately, more
              damaging to the academic enterprise. Using an anonymous
              elicitation format supplemented by incentives for honest
              reporting, we surveyed over 2,000 psychologists about their
              involvement in questionable research practices. The impact of
              truth-telling incentives on self-admissions of questionable
              research practices was positive, and this impact was greater for
              practices that respondents judged to be less defensible.
              Combining three different estimation methods, we found that the
              percentage of respondents who have engaged in questionable
              practices was surprisingly high. This finding suggests that some
              questionable practices may constitute the prevailing research
              norm.},
	author = {John, Leslie K and Loewenstein, George and Prelec, Drazen},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Psychol. Sci.},
	keywords = {trust\_us},
	language = {en},
	month = may,
	number = 5,
	pages = {524--532},
	title = {Measuring the prevalence of questionable research practices with incentives for truth telling},
	volume = 23,
	year = 2012}

@article{Kappes2014-wm,
	abstract = {{\ldots} The Reproducibility Project is an open collaboration to which
               anyone can contribute {\ldots} psychological sciences , we selected a
               quasi-random sample of studies from three prominent {\ldots}
               Psychology: Learning, Memory, and Cognition, and Psychological
               Science ) from the 2008 {\ldots}},
	author = {Kappes, Heather Barry and Collaboration, Open Science and {Others}},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	publisher = {CRC Press, Taylor \& Francis Group},
	title = {The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility},
	year = 2014}

@article{Key2016-vr,
	abstract = {ABSTRACTData access and research transparency (DA-RT) is a
               growing concern for the discipline. Technological advances have
               greatly reduced the cost of sharing data, enabling full
               replication archives consisting of data and code to be shared on
               individual websites, as well as journal archives and
               institutional data repositories. But how do we ensure that
               scholars take advantage of these resources to share their
               replication archives? Moreover, are the costs of research
               transparency borne by individuals or by journals? This article
               assesses the impact of journal replication policies on data
               availability and finds that articles published in journals with
               mandatory provision policies are 24 times more likely to have
               replication materials available than articles those with no
               requirements.},
	author = {Key, Ellen M},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	keywords = {trust\_us;thesis\_experiment},
	month = apr,
	number = 02,
	pages = {268--272},
	publisher = {Cambridge University Press (CUP)},
	title = {How are we doing? Data access and replication in political science},
	volume = 49,
	year = 2016}

@article{King1990-hy,
	abstract = {``Politimetrics'' (Gurr 1972), ``polimetrics,'' (Alker 1975),
               ``politometrics'' (Hilton 1976), ``political arithmetic'' (Petty
               [1672] 1971), ``quantitative Political Science (QPS),''
               ``governmetrics,'' ``posopolitics'' (Papayanopoulos 1973),
               ``political science statistics'' (Rai and Blydenburgh 1973),
               ``political statistics'' (Rice 1926). These are some of the
               names that scholars have used to describe the field we now call
               ``political methodology.''1 The history of political methodology
               has been quite fragmented until recently, as reflected by this
               patchwork of names. The field has begun to coalesce during the
               past decade; we are developing persistent organizations, a
               growing body of scholarly literature, and an emerging consensus
               about important problems that need to be solved.},
	author = {King, Gary},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Polit. Anal.},
	keywords = {trust\_us},
	pages = {1--29},
	publisher = {Cambridge University Press},
	title = {On Political Methodology},
	volume = 2,
	year = 1990}

@article{King2003-zv,
	abstract = {{\ldots} research in the natural sciences , and to some extent research
               in the social sciences ----and the {\ldots} VDC software will always be
               available since it is legally and permanently '' open source,''
               which {\ldots} user base, and encompass a larger and larger fraction of
               available social science data {\ldots}},
	author = {King, Gary},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {International Studies Perspectives},
	publisher = {Wiley-Blackwell},
	title = {The future of replication},
	year = 2003}

@misc{Kovyliaeva2019-fj,
	abstract = {{\ldots} rising tide of scientific data'' for the European
                  Commission (European Union, 2010). In this {\ldots} this standard
                  would allow to make further development of the social
                  sciences , help to build on {\ldots} DA-RT contributed to the
                  production of the Center for Open Science's Transparency and
                  {\ldots}},
	author = {Kovyliaeva, Natalia},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	howpublished = {\url{http://www.etd.ceu.edu/2019/kovyliaeva_natalia.pdf}},
	note = {Accessed: 2021-7-15},
	publisher = {etd.ceu.edu},
	title = {Replication crisis? The role of reviewers and data availability policies in political science journals},
	year = 2019}

@article{Krishna2021-ug,
	abstract = {There has been much debate on the need for preregistering
               experimental studies. Opinions differ with some people believing
               that ?pre-registration should be required?for us to believe the
               results of papers,? and others believing that ?pre-registration
               makes no difference to science and just adds work.? This
               research dialogue brings differing viewpoints together, in an
               open academic dialogue. Two target articles and two commentaries
               discuss what pre-registration does for replicability of studies,
               and what cost it adds on authors and reviewers. Additionally, in
               my introduction to the research dialogue, I deliberate on the
               need for synergy between our journals, review teams, authors,
               and institutions, in order for any new policies on
               pre-registration to be successfully adopted.},
	author = {Krishna, Aradhna},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {J. Consum. Psychol.},
	keywords = {trust\_us},
	language = {en},
	month = jan,
	number = 1,
	pages = {146--150},
	publisher = {Wiley},
	title = {The need for synergy in academic policies: An introduction to the dialogue on preregistration},
	volume = 31,
	year = 2021}

@article{Kristal2020-rh,
	abstract = {Honest reporting is essential for society to function well.
              However, people frequently lie when asked to provide information,
              such as misrepresenting their income to save money on taxes. A
              landmark finding published in PNAS [L. L. Shu, N. Mazar, F. Gino,
              D. Ariely, M. H. Bazerman, Proc. Natl. Acad. Sci. U.S.A. 109,
              15197-15200 (2012)] provided evidence for a simple way of
              encouraging honest reporting: asking people to sign a veracity
              statement at the beginning instead of at the end of a self-report
              form. Since this finding was published, various government
              agencies have adopted this practice. However, in this project, we
              failed to replicate this result. Across five conceptual
              replications (n = 4,559) and one highly powered, preregistered,
              direct replication (n = 1,235) conducted with the authors of the
              original paper, we observed no effect of signing first on honest
              reporting. Given the policy applications of this result, it is
              important to update the scientific record regarding the veracity
              of these results.},
	author = {Kristal, Ariella S and Whillans, Ashley V and Bazerman, Max H and Gino, Francesca and Shu, Lisa L and Mazar, Nina and Ariely, Dan},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	keywords = {morality; nudge; policy-making; replication;trust\_us},
	language = {en},
	month = mar,
	number = 13,
	pages = {7103--7107},
	title = {Signing at the beginning versus at the end does not decrease dishonesty},
	volume = 117,
	year = 2020}

@article{L_Haven2019-wh,
	abstract = {The threat to reproducibility and awareness of current rates of
               research misbehavior sparked initiatives to better academic
               science. One initiative is preregistration of quantitative
               research. We investigate whether the preregistration format
               could also be used to boost the credibility of qualitative
               research. A crucial distinction underlying preregistration is
               that between prediction and postdiction. In qualitative
               research, data are used to decide which way interpretation
               should move forward, using data to generate hypotheses and new
               research questions. Qualitative research is thus a real-life
               example of postdiction research. Some may object to the idea of
               preregistering qualitative studies because qualitative research
               generally does not test hypotheses, and because qualitative
               research design is typically flexible and subjective. We rebut
               these objections, arguing that making hypotheses explicit is
               just one feature of preregistration, that flexibility can be
               tracked using preregistration, and that preregistration would
               provide a check on subjectivity. We then contextualize
               preregistrations alongside another initiative to enhance
               credibility in qualitative research: the confirmability audit.
               Besides, preregistering qualitative studies is practically
               useful to combating dissemination bias and could incentivize
               qualitative researchers to report constantly on their study's
               development. We conclude with suggested modifications to the
               Open Science Framework preregistration form to tailor it for
               qualitative studies.},
	author = {L Haven, Tamarinde and Van Grootel, Dr Leonie},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Account. Res.},
	keywords = {Preregistration; qualitative research; transparency},
	language = {en},
	month = apr,
	number = 3,
	pages = {229--244},
	publisher = {Taylor \& Francis},
	title = {Preregistering qualitative research},
	volume = 26,
	year = 2019}

@article{LaCour2014-ul,
	abstract = {Can a single conversation change minds on divisive social issues,
              such as same-sex marriage? A randomized placebo-controlled trial
              assessed whether gay (n = 22) or straight (n = 19) messengers
              were effective at encouraging voters (n = 972) to support
              same-sex marriage and whether attitude change persisted and
              spread to others in voters' social networks. The results,
              measured by an unrelated panel survey, show that both gay and
              straight canvassers produced large effects initially, but only
              gay canvassers' effects persisted in 3-week, 6-week, and 9-month
              follow-ups. We also find strong evidence of within-household
              transmission of opinion change, but only in the wake of
              conversations with gay canvassers. Contact with gay canvassers
              further caused substantial change in the ratings of gay men and
              lesbians more generally. These large, persistent, and contagious
              effects were confirmed by a follow-up experiment. Contact with
              minorities coupled with discussion of issues pertinent to them is
              capable of producing a cascade of opinion change.},
	author = {LaCour, Michael J and Green, Donald P},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Science},
	language = {en},
	month = dec,
	number = 6215,
	pages = {1366--1369},
	title = {Political science. When contact changes minds: an experiment on transmission of support for gay equality},
	volume = 346,
	year = 2014}

@unpublished{Lal2021-qa,
	abstract = {Instrumental variable (IV) strategies are commonly used in
              political science to establish causal relationships, yet the
              identifying assumptions required by an IV design are demanding
              and it remains challenging for researchers to evaluate their
              plausibility. We replicate 61 papers published in three top
              journals in political science from the past decade (2011-2020)
              and document several troubling patterns: (1) researchers often
              miscalculate the first-stage F statistics, overestimating the
              strength of their IVs; (2) most researchers rely on classical
              asymptotic standard errors, which often severely underestimate
              the uncertainties around the two-stage-least-squared (2SLS)
              estimates; (3) in the majority of the replicated studies, the
              2SLS estimates are much bigger than the ordinary-least-squared
              estimates, and their ratio is negatively correlated with the
              strength of the IVs in studies where the IVs are not
              experimentally generated, suggesting potential violations of the
              exclusion restriction; such a relationship is much weaker with
              experimentally generated IVs. To improve practice, we provide a
              checklist for researchers to avoid these pitfalls and recommend a
              zero-first-stage test and a local-to-zero procedure to guard
              against failure of the identifying assumptions.},
	author = {Lal, Apoorva and Lockhart, Mackenzie William and Xu, Yiqing and Zu, Ziwen},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {instrumental variables, two-stage-least-squared, replications, weak IV, exclusion restriction, zero-first-stage test;trust\_us},
	month = aug,
	title = {How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice based on Over 60 Replicated Studies},
	year = 2021}

@misc{Leif2021-fo,
	abstract = {This post is co-authored with a team of researchers who have
                  chosen to remain anonymous. They uncovered most of the
                  evidence reported in this post. These researchers are not
                  connected in any way to the papers described herein. *** In
                  2012, Shu, Mazar, Gino, Ariely, and Bazerman published a
                  three-study paper in PNAS (.htm) reporting...},
	author = {Nelson, Leif D and Simonsohn, Uri and Simmons, Joseph P},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	howpublished = {\url{http://datacolada.org/98}},
	keywords = {trust\_us},
	language = {en},
	month = aug,
	note = {Accessed: 2021-8-18},
	title = {[98] Evidence of Fraud in an Influential Field Experiment About Dishonesty},
	year = 2021}

@article{Lenz2021-kk,
	abstract = {How often do articles depend on suppression effects for their
               findings? How often do they disclose this fact? By suppression
               effects, we mean control-variable-induced increases in estimated
               effect sizes. Researchers generally scrutinize suppression
               effects as they want reassurance that authors have a strong
               explanation for them, especially when the statistical
               significance of the key finding depends on them. In a reanalysis
               of observational studies from a leading journal, we find that
               over 30\% of articles depend on suppression effects for
               statistical significance. Although increases in key effect
               estimates from including control variables are of course
               potentially justifiable, none of the articles justify or
               disclose them. These findings may point to a hole in the review
               process: journals are accepting articles that depend on
               suppression effects without readers, reviewers, or editors being
               made aware.},
	author = {Lenz, Gabriel S and Sahn, Alexander},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Polit. Anal.},
	keywords = {reproducibility; transparency; open science; covariates; controls; suppression effects; regression},
	month = jul,
	number = 3,
	pages = {356--369},
	publisher = {Cambridge University Press},
	title = {Achieving Statistical Significance with Control Variables and Without Transparency},
	volume = 29,
	year = 2021}

@article{Levenstein2018-xv,
	abstract = {Data sharing promotes scientific progress by permitting
               replication of prior scientific analyses and by increasing the
               return on the human and financial investments made in data
               collection. The costs of data sharing can be reduced through the
               implementation of best practices in data management across the
               research life cycle; this article provides specific guidance on
               these practices. The benefits of data sharing will be reaped
               when researchers who share their data are rewarded with
               citations and recognition of the intellectual value inherent in
               producing new scientific data.},
	author = {Levenstein, Margaret C and Lyle, Jared A},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Advances in Methods and Practices in Psychological Science},
	month = mar,
	number = 1,
	pages = {95--103},
	publisher = {SAGE Publications Inc},
	title = {Data: Sharing Is Caring},
	volume = 1,
	year = 2018}

@article{McKiernan2016-un,
	abstract = {{\ldots} The IF is a flawed metric that correlates poorly with the
               scientific quality of indi- vidual articles (Brembs et al.,
               2013; Neuberger and Counsell, 2002; PLOS Medicine Editors, 2006;
               Seglen, 1997) {\ldots} Open Science Framework osf.io {\ldots} biological,
               life, medical, and computer sciences {\ldots}},
	author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and {Others}},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Elife},
	pages = {e16800},
	publisher = {eLife Sciences Publications Limited},
	title = {Point of view: How open science helps researchers succeed},
	volume = 5,
	year = 2016}

@unpublished{Mellor2018-jj,
	abstract = {Background and Perspective This is an exciting time to be a
              psychological scientist. There is a major new movement that seeks
              to promote the credibility and replicability of psychological
              research by enhancing its transparency, with scholarly societies
              promoting the principles
              (http://www.psychologicalscience.org/publications/open-science)
              and groups formed specifically to advance that mission (see
              http://improvingpsych.org/ and https://cos.io for two examples).
              While relatively low rates of replicability among scientific
              findings (Begley \& Ellis, 2012; OSC, 2015; Chang \& Li 2015)
              inspired the existence of these groups, in this chapter we
              describe how striving to maximize transparency in your research
              can benefit both science and your career.},
	author = {Mellor, David T and Vazire, Simine and Lindsay, D S},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {methods; open data; open science; power; preprints; preregistration; transparency},
	month = jan,
	title = {Transparent science: A more credible, reproducible, and publishable way to do science},
	year = 2018}

@article{Michael_Alvarez2018-ns,
	abstract = {With the discipline's push toward data access and research
               transparency (DA-RT), journal replication archives are becoming
               increasingly common. As researchers work to ensure that
               replication materials are provided, they also should pay
               attention to the content---rather than simply the provision---of
               journal archives. Based on our experience in analyzing and
               handling journal replication materials, we present a series of
               recommendations that can make them easier to understand and use.
               The provision of clear, functional, and well-documented
               replication materials is key for achieving the goals of
               transparent and replicable research. Furthermore, good
               replication materials enhance the development of extensions and
               related research by making state-of-the-art methodologies and
               analyses more accessible.},
	author = {Michael Alvarez, R and Key, Ellen M and N{\'u}{\~n}ez, Lucas},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = apr,
	number = 2,
	pages = {422--426},
	publisher = {Cambridge University Press},
	title = {Research Replication: Practical Considerations},
	volume = 51,
	year = 2018}

@article{Miguel2014-to,
	author = {Miguel, E and Camerer, C and Casey, K and Cohen, J and {others}},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	publisher = {science.sciencemag.org},
	title = {Promoting transparency in social science research},
	year = 2014}

@article{Monogan2015-ez,
	abstract = {This article describes the current debate on the practice of
               preregistration in political science---that is, publicly
               releasing a research design before observing outcome data. The
               case in favor of preregistration maintains that it can restrain
               four potential causes of publication bias, clearly distinguish
               deductive and inductive studies, add transparency regarding a
               researcher's motivation, and liberate researchers who may be
               pressured to find specific results. Concerns about
               preregistration maintain that it is less suitable for the study
               of historical data, could reduce data exploration, may not allow
               for contextual problems that emerge in field research, and may
               increase the difficulty of finding true positive results. This
               article makes the case that these concerns can be addressed in
               preregistered studies, and it offers advice to those who would
               like to pursue study registration in their own work.},
	author = {Monogan, James E},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	keywords = {trust\_us},
	month = jul,
	number = 3,
	pages = {425--429},
	publisher = {Cambridge University Press},
	title = {Research Preregistration in Political Science: The Case, Counterarguments, and a Response to Critiques},
	volume = 48,
	year = 2015}

@article{Monogan2015-ia,
	abstract = {This article describes the current debate on the practice of
               preregistration in political science---that is, publicly
               releasing a research design before observing outcome data. The
               case in favor of preregistration maintains that it can restrain
               four potential causes of publication bias, clearly distinguish
               deductive and inductive studies, add transparency regarding a
               researcher's motivation, and liberate researchers who may be
               pressured to find specific results. Concerns about
               preregistration maintain that it is less suitable for the study
               of historical data, could reduce data exploration, may not allow
               for contextual problems that emerge in field research, and may
               increase the difficulty of finding true positive results. This
               article makes the case that these concerns can be addressed in
               preregistered studies, and it offers advice to those who would
               like to pursue study registration in their own work.},
	author = {Monogan, James E},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = jul,
	number = 3,
	pages = {425--429},
	publisher = {Cambridge University Press},
	title = {Research Preregistration in Political Science: The Case, Counterarguments, and a Response to Critiques},
	volume = 48,
	year = 2015}

@article{Mullinix2015-zu,
	abstract = {{\ldots} Finally, while our results differ from other replication
               efforts ( Open Science Collaboration 2015), it remains {\ldots}
               studies, the source of our population-based sample is the
               National Science Foundation funded Time-sharing Experiments for
               the Social Sciences (TESS) program {\ldots}},
	author = {Mullinix, Kevin J and Leeper, Thomas J and Druckman, James N and Freese, Jeremy},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Journal of Experimental Political Science},
	number = 2,
	pages = {109--138},
	publisher = {Cambridge University Press},
	title = {The generalizability of survey experiments},
	volume = 2,
	year = 2015}

@unpublished{Neve2021-fb,
	abstract = {Sharing data has many benefits. However, data sharing rates
              remain low, for the most part well below 50\%. A variety of
              interventions encouraging data sharing have been proposed. We
              focus here on editorial policies. Kidwell et al. (2016) assessed
              the impact of the introduction of badges in Psychological
              Science; Hardwicke et al. (2018) assessed the impact of
              Cognition's mandatory data sharing policy. Both studies found
              policies to improve data sharing practices, but only assessed the
              impact of the policy for up to 25 months after its
              implementation. We examined the effect of these policies over a
              longer term by reusing their data and collecting a follow-up
              sample including articles published up until December 31st, 2019.
              We fit generalized additive models as these allow for a flexible
              assessment of the effect of time, in particular to identify
              non-linear changes in the trend. These models were compared to
              generalized linear models to examine whether the non-linearity is
              needed. Descriptive results and the outputs from generalized
              additive and linear models were coherent with previous findings:
              following the policies in Cognition and Psychological Science,
              data sharing statement rates increased immediately and continued
              to increase beyond the timeframes examined previously, until
              reaching close to 100\%. In Clinical Psychological Science, data
              sharing statement rates started to increase only two years
              following the implementation of badges. Reusability rates jumped
              from close to 0\% to around 50\% but did not show changes within
              the pre-policy nor the post-policy timeframes. Journals that did
              not implement a policy showed no change in data sharing rates or
              reusability over time. There was variability across journals in
              the levels of increase, so we suggest future research should
              examine a larger number of policies to draw conclusions about
              their efficacy. We also encourage future research to investigate
              the barriers to data sharing specific to psychology subfields to
              identify the best interventions to tackle them.},
	author = {Neve, Judith and Rousselet, Guillaume A},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PsyArXiv},
	keywords = {trust\_us},
	month = jun,
	title = {The evolution of data sharing practices in the psychological literature},
	year = 2021}

@unpublished{Neve2021-xo,
	abstract = {Sharing data has many benefits. However, data sharing rates
              remain low, for the most part well below 50\%. A variety of
              interventions encouraging data sharing have been proposed. We
              focus here on editorial policies. Kidwell et al. (2016) assessed
              the impact of the introduction of badges in Psychological
              Science; Hardwicke et al. (2018) assessed the impact of
              Cognition's mandatory data sharing policy. Both studies found
              policies to improve data sharing practices, but only assessed the
              impact of the policy for up to 25 months after its
              implementation. We examined the effect of these policies over a
              longer term by reusing their data and collecting a follow-up
              sample including articles published up until December 31st, 2019.
              We fit generalized additive models as these allow for a flexible
              assessment of the effect of time, in particular to identify
              non-linear changes in the trend. These models were compared to
              generalized linear models to examine whether the non-linearity is
              needed. Descriptive results and the outputs from generalized
              additive and linear models were coherent with previous findings:
              following the policies in Cognition and Psychological Science,
              data sharing statement rates increased immediately and continued
              to increase beyond the timeframes examined previously, until
              reaching close to 100\%. In Clinical Psychological Science, data
              sharing statement rates started to increase only two years
              following the implementation of badges. Reusability rates jumped
              from close to 0\% to around 50\% but did not show changes within
              the pre-policy nor the post-policy timeframes. Journals that did
              not implement a policy showed no change in data sharing rates or
              reusability over time. There was variability across journals in
              the levels of increase, so we suggest future research should
              examine a larger number of policies to draw conclusions about
              their efficacy. We also encourage future research to investigate
              the barriers to data sharing specific to psychology subfields to
              identify the best interventions to tackle them.},
	author = {Neve, Judith and Rousselet, Guillaume A},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {journal policy; metapsychology; metascience; open science; reproducibility; scientific publishing},
	month = jun,
	title = {The Evolution of Data Sharing Practices in the Psychological Literature},
	year = 2021}

@article{Nosek2014-lb,
	abstract = {{\ldots} doi: 10.1177/1745691612462588 First citation in
               articleCrossref, Google Scholar. Open Science Collaboration .
               (2013) {\ldots} The powerful concept of replication is neglected in the
               social sciences . Review of General Psychology, 13, 90--100 {\ldots}
               Psychological Science , 22, 1359--1366 {\ldots}},
	author = {Nosek, Brian A and Lakens, Dani{\"e}l},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Soc. Psychol.},
	month = may,
	number = 3,
	pages = {137--141},
	publisher = {Hogrefe Publishing},
	title = {Registered Reports},
	volume = 45,
	year = 2014}

@article{Nosek2018-an,
	abstract = {Progress in science relies in part on generating hypotheses with
              existing observations and testing hypotheses with new
              observations. This distinction between postdiction and prediction
              is appreciated conceptually but is not respected in practice.
              Mistaking generation of postdictions with testing of predictions
              reduces the credibility of research findings. However, ordinary
              biases in human reasoning, such as hindsight bias, make it hard
              to avoid this mistake. An effective solution is to define the
              research questions and analysis plan before observing the
              research outcomes-a process called preregistration.
              Preregistration distinguishes analyses and outcomes that result
              from predictions from those that result from postdictions. A
              variety of practical strategies are available to make the best
              possible use of preregistration in circumstances that fall short
              of the ideal application, such as when the data are preexisting.
              Services are now available for preregistration across all
              disciplines, facilitating a rapid increase in the practice.
              Widespread adoption of preregistration will increase
              distinctiveness between hypothesis generation and hypothesis
              testing and will improve the credibility of research findings.},
	author = {Nosek, Brian A and Ebersole, Charles R and DeHaven, Alexander C and Mellor, David T},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	keywords = {confirmatory analysis; exploratory analysis; methodology; open science; preregistration;trust\_us},
	language = {en},
	month = mar,
	number = 11,
	pages = {2600--2606},
	title = {The preregistration revolution},
	volume = 115,
	year = 2018}

@article{Nyhan2015-cs,
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096515000463/resource/name/firstPage-S1049096515000463a.jpg},
	author = {Nyhan, Brendan},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = sep,
	number = {S1},
	pages = {78--83},
	publisher = {Cambridge University Press},
	title = {Increasing the Credibility of Political Science Research: A Proposal for Journal Reforms},
	volume = 48,
	year = 2015}

@article{Open_Science_Collaboration2015-gk,
	abstract = {Reproducibility is a defining feature of science, but the extent
              to which it characterizes current research is unknown. We
              conducted replications of 100 experimental and correlational
              studies published in three psychology journals using high-powered
              designs and original materials when available. Replication
              effects were half the magnitude of original effects, representing
              a substantial decline. Ninety-seven percent of original studies
              had statistically significant results. Thirty-six percent of
              replications had statistically significant results; 47\% of
              original effect sizes were in the 95\% confidence interval of the
              replication effect size; 39\% of effects were subjectively rated
              to have replicated the original result; and if no bias in
              original results is assumed, combining original and replication
              results left 68\% with statistically significant effects.
              Correlational tests suggest that replication success was better
              predicted by the strength of original evidence than by
              characteristics of the original and replication teams.},
	author = {{Open Science Collaboration}},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Science},
	keywords = {trust\_us;thesis\_experiment},
	language = {en},
	month = aug,
	number = 6251,
	pages = {aac4716},
	title = {{PSYCHOLOGY}. Estimating the reproducibility of psychological science},
	volume = 349,
	year = 2015}

@article{Penders2019-ri,
	abstract = {The increasing pursuit of replicable research and actual
               replication of research is a political project that articulates
               a very specific technology of accountability for science. This
               project was initiated in response to concerns about the openness
               and trustworthiness of science. Though applicable and valuable
               in many fields, here we argue that this value cannot be extended
               everywhere, since the epistemic content of fields, as well as
               their accountability infrastructures, differ. Furthermore, we
               argue that there are limits to replicability across all fields;
               but in some fields, including parts of the humanities, these
               limits severely undermine the value of replication to account
               for the value of research.},
	author = {Penders, Bart and Holbrook, J Britt and de Rijcke, Sarah},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Publications},
	language = {en},
	month = jul,
	number = 3,
	pages = {52},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Rinse and Repeat: Understanding the Value of Replication across Different Ways of Knowing},
	volume = 7,
	year = 2019}

@article{Pham2021-xr,
	abstract = {To address widespread perceptions of a reproducibility crisis
                 in the social sciences, a growing number of scholars recommend
                 the systematic preregistration of empirical studies. The
                 purpose of this article is to contribute to an epistemological
                 dialogue on the value of preregistration in consumer research
                 by identifying the limitations, drawbacks, and potential
                 adverse effects of a preregistration system. After a brief
                 review of some of the implementation challenges that commonly
                 arise with preregistration, we raise three levels of issues
                 with a system of preregistration. First, we identify its
                 limitations as a means of advancing consumer knowledge, thus
                 questioning the sufficiency of preregistration in promoting
                 good consumer science. Second, we elaborate on why consumer
                 science can progress even in the absence of preregistration,
                 thereby also questioning the necessity of preregistration in
                 promoting good consumer science. Third, we discuss serious
                 potential adverse effects of preregistration, both at the
                 individual researcher level and at the level of the field as a
                 whole. We conclude by offering a broader perspective on the
                 narrower role that preregistration can play within the general
                 pursuit of building robust and useful knowledge about
                 consumers.},
	author = {Pham, Michel Tuan and Oh, Travis Tae},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {J. Consum. Psychol.},
	keywords = {trust\_us},
	language = {en},
	month = jan,
	number = 1,
	original_id = {6d880f96-bdc8-0bcd-8e4d-907b36ee0908},
	pages = {163--176},
	publisher = {Wiley},
	title = {Preregistration is neither sufficient nor necessary for good science},
	volume = 31,
	year = 2021}

@article{Recker2015-qf,
	abstract = {{\ldots} | Paving the Way for Data-Centric, Open Science jlsc-pub.org {\ldots}
               Digital preservation policy. Principles of digital preservation
               at the Data Archive for the Social Sciences {\ldots} Proceedings of the
               2014 ACM Conference on Web Science , 91 98.
               http://dx.doi.org/10.1145/2615569.2615685 {\ldots}},
	author = {Recker, Astrid and M{\"u}ller, Stefan and Trixa, Jessica and Schumann, Natascha},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Journal of Librarianship and Scholarly Communication},
	number = 2,
	publisher = {Iowa State University Digital Press},
	title = {Paving the Way For {Data-Centric}, Open Science: An Example From the Social Sciences},
	volume = 3,
	year = 2015}

@article{Sayre2018-ht,
	abstract = {In recent years, evidence has emerged from disciplines ranging
               from biology to economics that many scientific studies are not
               reproducible. This evidence has led to declarations in both the
               scientific and lay press that science is experiencing a
               ``reproducibility crisis'' and that this crisis has significant
               impacts on both science and society, including misdirected
               effort, funding, and policy implemented on the basis of
               irreproducible research. In many cases, academic libraries are
               the natural organizations to lead efforts to implement
               recommendations from journals, funders, and societies to improve
               research reproducibility. In this editorial, we introduce the
               reproducibility crisis, define reproducibility and
               replicability, and then discusses how academic libraries can
               lead institutional support for reproducible research.},
	author = {Sayre, Franklin and Riegelman, Amy},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Coll. Res. Libr.},
	language = {en},
	month = jan,
	number = 1,
	pages = {2},
	publisher = {journals.acrl.org},
	title = {The Reproducibility Crisis and Academic Libraries},
	volume = 79,
	year = 2018}

@article{Schmidt2021-fq,
	abstract = {{\ldots} and secure knowledge, this procedure was---until
               recently---very unpopular in the social sciences {\ldots} Replication
               studies were difficult to publish; scientific careers are built
               on genuineness and {\ldots} of the Reproducibility Proj- ect by Brian
               Nosek and the Open Science Collaboration {\ldots}},
	author = {Schmidt, Stefan},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Research Methods in the Social Sciences: an AZ of Key Concepts},
	pages = {238},
	publisher = {Oxford University Press, USA},
	title = {{REPLICATION} {AND} {REPRODUCIBILITY}},
	year = 2021}

@article{Shrout2018-wq,
	abstract = {Psychology advances knowledge by testing statistical hypotheses
               using empirical observations and data. The expectation is that
               most statistically significant findings can be replicated in new
               data and in new laboratories, but in practice many findings have
               replicated less often than expected, leading to claims of a
               replication crisis. We review recent methodological literature
               on questionable research practices, meta-analysis, and power
               analysis to explain the apparently high rates of failure to
               replicate. Psychologists can improve research practices to
               advance knowledge in ways that improve replicability. We
               recommend that researchers adopt open science conventions of
               preregi-stration and full disclosure and that replication
               efforts be based on multiple studies rather than on a single
               replication attempt. We call for more sophisticated power
               analyses, careful consideration of the various influences on
               effect sizes, and more complete disclosure of nonsignificant as
               well as statistically significant findings.},
	author = {Shrout, Patrick E and Rodgers, Joseph L},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Annu. Rev. Psychol.},
	keywords = {methodology; replication; statistics},
	language = {en},
	month = jan,
	pages = {487--510},
	publisher = {annualreviews.org},
	title = {Psychology, Science, and Knowledge Construction: Broadening Perspectives from the Replication Crisis},
	volume = 69,
	year = 2018}

@misc{Shu2012-nr,
	author = {Shu, Lisa L and Mazar, Nina and Gino, Francesca and Ariely, Dan and Bazerman, Max H},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Proceedings of the National Academy of Sciences},
	number = 38,
	pages = {15197--15200},
	title = {Signing at the beginning makes ethics salient and decreases dishonest self-reports in comparison to signing at the end},
	volume = 109,
	year = 2012}

@article{Silberzahn2018-rw,
	abstract = {Twenty-nine teams involving 61 analysts used the same data set
               to address the same research question: whether soccer referees
               are more likely to give red cards to dark-skin-toned players
               than to light-skin-toned players. Analytic approaches varied
               widely across the teams, and the estimated effect sizes ranged
               from 0.89 to 2.93 ( Mdn = 1.31) in odds-ratio units. Twenty
               teams (69\%) found a statistically significant positive effect,
               and 9 teams (31\%) did not observe a significant relationship.
               Overall, the 29 different analyses used 21 unique combinations
               of covariates. Neither analysts' prior beliefs about the effect
               of interest nor their level of expertise readily explained the
               variation in the outcomes of the analyses. Peer ratings of the
               quality of the analyses also did not account for the
               variability. These findings suggest that significant variation
               in the results of analyses of complex data may be difficult to
               avoid, even by experts with honest intentions. Crowdsourcing
               data analysis, a strategy in which numerous research teams are
               recruited to simultaneously investigate the same research
               question, makes transparent how defensible, yet subjective,
               analytic choices influence research results.},
	author = {Silberzahn, R and Uhlmann, E L and Martin, D P and Anselmi, P and Aust, F and Awtrey, E and Bahn{\'\i}k, {\v S} and Bai, F and Bannard, C and Bonnier, E and Carlsson, R and Cheung, F and Christensen, G and Clay, R and Craig, M A and Dalla Rosa, A and Dam, L and Evans, M H and Flores Cervantes, I and Fong, N and Gamez-Djokic, M and Glenz, A and Gordon-McKeon, S and Heaton, T J and Hederos, K and Heene, M and Hofelich Mohr, A J and H{\"o}gden, F and Hui, K and Johannesson, M and Kalodimos, J and Kaszubowski, E and Kennedy, D M and Lei, R and Lindsay, T A and Liverani, S and Madan, C R and Molden, D and Molleman, E and Morey, R D and Mulder, L B and Nijstad, B R and Pope, N G and Pope, B and Prenoveau, J M and Rink, F and Robusto, E and Roderique, H and Sandberg, A and Schl{\"u}ter, E and Sch{\"o}nbrodt, F D and Sherman, M F and Sommer, S A and Sotak, K and Spain, S and Sp{\"o}rlein, C and Stafford, T and Stefanutti, L and Tauber, S and Ullrich, J and Vianello, M and Wagenmakers, E-J and Witkowiak, M and Yoon, S and Nosek, B A},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Adv. Methods Pract. Psychol. Sci.},
	keywords = {trust\_us},
	language = {en},
	month = sep,
	number = 3,
	pages = {337--356},
	publisher = {SAGE Publications},
	title = {Many analysts, one data set: Making transparent how variations in analytic choices affect results},
	volume = 1,
	year = 2018}

@article{Simmons2021-my,
	abstract = {We identify 15 claims Pham and Oh (2020) make to argue against
               pre-registration. We agree with 7 of the claims, but think that
               none of them justify delaying the encouragement and adoption of
               pre-registration. Moreover, while the claim they make in their
               title is correct?pre-registration is neither necessary nor
               sufficient for a credible science?this is also true of many our
               science?s most valuable tools, such as random assignment.
               Indeed, both random assignment and pre-registration lead to more
               credible research. Pre-registration is a game changer.},
	author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {J. Consum. Psychol.},
	keywords = {trust\_us},
	language = {en},
	month = jan,
	number = 1,
	pages = {177--180},
	publisher = {Wiley},
	title = {Preregistration is a Game Changer. But, Like Random Assignment, it is Neither Necessary Nor Sufficient for Credible Science},
	volume = 31,
	year = 2021}

@article{Simmons2021-qm,
	abstract = {In this article, we (1) discuss the reasons why pre-registration
               is a good idea, both for the field and individual researchers,
               (2) respond to arguments against pre-registration, (3) describe
               how to best write and review a pre-registration, and (4) comment
               on pre-registration?s rapidly accelerating popularity. Along the
               way, we describe the (big) problem that pre-registration can
               solve (i.e., false positives caused by p-hacking), while also
               offering viable solutions to the problems that pre-registration
               cannot solve (e.g., hidden confounds or fraud). Pre-registration
               does not guarantee that every published finding will be true,
               but without it you can safely bet that many more will be false.
               It is time for our field to embrace pre-registration, while
               taking steps to ensure that it is done right.},
	author = {Simmons, Joseph and Nelson, Leif and Simonsohn, Uri},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {J. Consum. Psychol.},
	keywords = {trust\_us;thesis\_experiment},
	language = {en},
	month = jan,
	number = 1,
	pages = {151--162},
	publisher = {Wiley},
	title = {Preregistration: Why and how},
	volume = 31,
	year = 2021}

@article{Simonsohn2013-fa,
	abstract = {I argue that requiring authors to post the raw data supporting
              their published results has the benefit, among many others, of
              making fraud much less likely to go undetected. I illustrate this
              point by describing two cases of suspected fraud I identified
              exclusively through statistical analysis of reported means and
              standard deviations. Analyses of the raw data behind these
              published results provided invaluable confirmation of the initial
              suspicions, ruling out benign explanations (e.g., reporting
              errors, unusual distributions), identifying additional signs of
              fabrication, and also ruling out one of the suspected fraud's
              explanations for his anomalous results. If journals, granting
              agencies, universities, or other entities overseeing research
              promoted or required data posting, it seems inevitable that fraud
              would be reduced.},
	author = {Simonsohn, Uri},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Psychol. Sci.},
	keywords = {data posting; data sharing; decision making; fake data; judgment; scientific communication;trust\_us},
	language = {en},
	month = oct,
	number = 10,
	pages = {1875--1888},
	title = {Just post it: the lesson from two cases of fabricated data detected by statistics alone},
	volume = 24,
	year = 2013}

@article{Simonsohn2014-qg,
	abstract = {Journals tend to publish only statistically significant evidence,
              creating a scientific record that markedly overstates the size of
              effects. We provide a new tool that corrects for this bias
              without requiring access to nonsignificant results. It
              capitalizes on the fact that the distribution of significant p
              values, p-curve, is a function of the true underlying effect.
              Researchers armed only with sample sizes and test results of the
              published findings can correct for publication bias. We validate
              the technique with simulations and by reanalyzing data from the
              Many-Labs Replication project. We demonstrate that p-curve can
              arrive at conclusions opposite that of existing tools by
              reanalyzing the meta-analysis of the ``choice overload''
              literature.},
	author = {Simonsohn, Uri and Nelson, Leif D and Simmons, Joseph P},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Perspect. Psychol. Sci.},
	keywords = {p-curve; p-hacking; publication bias;trust\_us},
	language = {en},
	month = nov,
	number = 6,
	pages = {666--681},
	title = {p-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results},
	volume = 9,
	year = 2014}

@article{Smaldino2016-gk,
	abstract = {Poor research design and data analysis encourage false-positive
               findings. Such poor methods persist despite perennial calls for
               improvement, suggesting that they result from something more
               than just misunderstanding. The persistence of poor methods
               results partly from incentives that favour them, leading to the
               natural selection of bad science. This dynamic requires no
               conscious strategizing-no deliberate cheating nor loafing-by
               scientists, only that publication is a principal factor for
               career advancement. Some normative methods of analysis have
               almost certainly been selected to further publication instead of
               discovery. In order to improve the culture of science, a shift
               must be made away from correcting misunderstandings and towards
               rewarding understanding. We support this argument with empirical
               evidence and computational modelling. We first present a 60-year
               meta-analysis of statistical power in the behavioural sciences
               and show that power has not improved despite repeated
               demonstrations of the necessity of increasing power. To
               demonstrate the logical consequences of structural incentives,
               we then present a dynamic model of scientific communities in
               which competing laboratories investigate novel or previously
               published hypotheses using culturally transmitted research
               methods. As in the real world, successful labs produce more
               'progeny,' such that their methods are more often copied and
               their students are more likely to start labs of their own.
               Selection for high output leads to poorer methods and
               increasingly high false discovery rates. We additionally show
               that replication slows but does not stop the process of
               methodological deterioration. Improving the quality of research
               requires change at the institutional level.},
	author = {Smaldino, Paul E and McElreath, Richard},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {R Soc Open Sci},
	keywords = {Campbell's Law; cultural evolution; incentives; metascience; replication; statistical power},
	language = {en},
	month = sep,
	number = 9,
	pages = {160384},
	publisher = {royalsocietypublishing.org},
	title = {The natural selection of bad science},
	volume = 3,
	year = 2016}

@unpublished{Spellman2017-xq,
	abstract = {Open Science is a collection of actions designed to make
              scientific processes more transparent and results more
              accessible. Its goal is to build a more replicable and robust
              science; it does so using new technologies, altering incentives,
              and changing attitudes. The current movement towards open science
              was spurred, in part, by a recent ``series of unfortunate
              events'' within psychology and other sciences. These events
              include the large number of studies that have failed to replicate
              and the prevalence of common research and publication procedures
              that could explain why. Many journals and funding agencies now
              encourage, require, or reward some open science practices,
              including pre-registration, providing full materials, posting
              data, distinguishing between exploratory and confirmatory
              analyses, and running replication studies. Individuals can
              practice and encourage open science in their many roles as
              researchers, authors, reviewers, editors, teachers, and members
              of hiring, tenure, promotion, and awards committees. A plethora
              of resources are available to help scientists, and science,
              achieve these goals.},
	author = {Spellman, Bobbie and Gilbert, Elizabeth A and Corker, Katherine S},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	keywords = {Data sharing; File Drawer Problem; Open Access; Open Science; Pre-registration; Questionable Research Practices; Replication Crisis; Reproducibility},
	month = apr,
	title = {Open Science: What, Why, and How},
	year = 2017}

@article{Stockemer2018-qg,
	abstract = {Do researchers share their quantitative data and are the
               quantitative results that are published in political science
               journals replicable? We attempt to answer these questions by
               analyzing all articles published in the 2015 issues of three
               political behaviorist journals (i.e., Electoral Studies, Party
               Politics, and Journal of Elections, Public Opinion \&
               Parties)---all of which did not have a binding data-sharing and
               replication policy as of 2015. We found that authors are still
               reluctant to share their data; only slightly more than half of
               the authors in these journals do so. For those who share their
               data, we mainly confirmed the initial results reported in the
               respective articles in roughly 70\% of the times. Only roughly
               5\% of the articles yielded significantly different results from
               those reported in the publication. However, we also found that
               roughly 25\% of the articles organized the data and/or code so
               poorly that replication was impossible.},
	author = {Stockemer, Daniel and Koehler, Sebastian and Lentz, Tobias},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	keywords = {trust\_us;thesis\_experiment},
	month = oct,
	number = 4,
	pages = {799--803},
	publisher = {Cambridge University Press},
	title = {Data Access, Transparency, and Replication: New Insights from the Political Behavior Literature},
	volume = 51,
	year = 2018}

@article{Stodden2011-yk,
	abstract = {{\ldots} I will touch on the semantic and substantive differences in
               the various approaches to reliability in computational and data-
               enabled sciences {\ldots} Science has never been about open data per
               se, but openness is something hard fought and won in the context
               of reproducibility {\ldots}},
	author = {Stodden, Victoria C},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	publisher = {academiccommons.columbia.edu},
	title = {Trust your science? Open your data and code},
	year = 2011}

@article{Stromland2019-ha,
	abstract = {Many view preregistration as a promising way to improve research
               credibility. However, scholars have argued that using
               pre-analysis plans in Experimental Economics has limited
               benefits. This paper argues that preregistration of studies is
               likely to improve research credibility. I show that in a setting
               with selective reporting and low statistical power, effect sizes
               are highly inflated, and this translates into low
               reproducibility. Preregistering the original studies could avoid
               such inflation of effect sizes---through increasing the share of
               ``frequentist'' researchers---and would lead to more credible
               power analyses for replication studies. Numerical applications
               of the model indicate that the inflation bias could be very
               large in practice, and available empirical evidence is in line
               with the central assumptions of the model.},
	author = {Str{\o}mland, Eirik},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {J. Econ. Psychol.},
	month = dec,
	pages = {102143},
	publisher = {Elsevier},
	title = {Preregistration and reproducibility},
	volume = 75,
	year = 2019}

@article{Swire2017-mt,
	abstract = {This study investigated the cognitive processing of true and
               false political information. Specifically, it examined the
               impact of source credibility on the assessment of veracity when
               information comes from a polarizing source (Experiment 1), and
               effectiveness of explanations when they come from one's own
               political party or an opposition party (Experiment 2). These
               experiments were conducted prior to the 2016 Presidential
               election. Participants rated their belief in factual and
               incorrect statements that President Trump made on the campaign
               trail; facts were subsequently affirmed and misinformation
               retracted. Participants then re-rated their belief immediately
               or after a delay. Experiment 1 found that (i) if information was
               attributed to Trump, Republican supporters of Trump believed it
               more than if it was presented without attribution, whereas the
               opposite was true for Democrats and (ii) although Trump
               supporters reduced their belief in misinformation items
               following a correction, they did not change their voting
               preferences. Experiment 2 revealed that the explanation's source
               had relatively little impact, and belief updating was more
               influenced by perceived credibility of the individual initially
               purporting the information. These findings suggest that people
               use political figures as a heuristic to guide evaluation of what
               is true or false, yet do not necessarily insist on veracity as a
               prerequisite for supporting political candidates.},
	author = {Swire, Briony and Berinsky, Adam J and Lewandowsky, Stephan and Ecker, Ullrich K H},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {R Soc Open Sci},
	keywords = {belief updating; continued influence effect; misinformation; motivated cognition; source credibility},
	language = {en},
	month = mar,
	number = 3,
	pages = {160802},
	publisher = {royalsocietypublishing.org},
	title = {Processing political misinformation: comprehending the Trump phenomenon},
	volume = 4,
	year = 2017}

@article{Tackett2020-ae,
	abstract = {English: Preregistration, which involves documentation of
               hypotheses, methods, and plans for data analysis prior to data
               collection or analysis, has been lauded as 1 potential solution
               to the replication crisis in psychological science. Yet, many
               researchers have been slow to adopt preregistration, and the
               next generation of researchers is offered little formalized
               instruction in creating comprehensive preregistrations. In this
               article, we describe a collaborative workshop-based
               preregistration course designed and taught by Jennifer L.
               Tackett. We provide a brief overview of preregistration,
               including resources available, common concerns with
               preregistration, and responses to these concerns. We then
               describe the goals, structure, and evolution of our
               preregistration course and provide examples of enrolled
               students' final research products. We conclude with reflections
               on the strengths and opportunities for growth for the 1st
               iteration of this course and suggestions for others who are
               interested in implementing similar open science--focused courses
               in their training programs. (PsycInfo Database Record (c) 2020
               APA, all rights reserved) French: La pr{\'e}inscription, qui
               comprend la documentation des hypoth{\`e}ses, des m{\'e}thodes
               et des plans d'analyse des donn{\'e}es avant la collecte ou
               l'analyse des donn{\'e}es, a {\'e}t{\'e} louang{\'e}e comme une
               solution potentielle {\`a} la crise de r{\'e}plication dans le
               domaine de la science psychologique. Pourtant, de nombreux
               chercheurs ont {\'e}t{\'e} lents {\`a} adopter la
               pr{\'e}inscription, et la prochaine g{\'e}n{\'e}ration de
               chercheurs se voit offrir peu d'instructions formelles
               concernant la cr{\'e}ation de pr{\'e}inscriptions compl{\`e}tes.
               Dans cet article, nous d{\'e}crivons un cours de
               pr{\'e}inscription donn{\'e} {\`a} partir d'ateliers de
               collaboration con{\c c}u et enseign{\'e} par Jennifer L.
               Tackett. Nous fournissons un aper{\c c}u de la
               pr{\'e}inscription, y compris les ressources disponibles, les
               pr{\'e}occupations courantes li{\'e}es {\`a} la
               pr{\'e}inscription et les r{\'e}ponses {\`a} ces
               pr{\'e}occupations. Nous d{\'e}crivons ensuite les objectifs, la
               structure et l'{\'e}volution de notre cours de
               pr{\'e}inscription et fournissons des exemples de produits de
               recherche finaux des {\'e}tudiants inscrits. Nous concluons par
               des r{\'e}flexions sur les points forts et les possibilit{\'e}s
               de croissance pour la premi{\`e}re it{\'e}ration de ce cours et
               des suggestions pour d'autres qui souhaitent mettre en {\oe}uvre
               des cours similaires ax{\'e}s sur la science ouverte dans leurs
               programmes de formation. (PsycInfo Database Record (c) 2020 APA,
               all rights reserved)},
	author = {Tackett, Jennifer L and Brandes, Cassandra M and Dworak, Elizabeth M and Shields, Allison N},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Canadian Psychology/Psychologie canadienne},
	month = nov,
	number = 4,
	pages = {299--309},
	publisher = {psycnet.apa.org},
	title = {Bringing the (pre)registration revolution to graduate training},
	volume = 61,
	year = 2020}

@article{Toth2021-qv,
	abstract = {{\ldots} Scientific research is most effective when it is open,
               transparent, and reproducible (Nosek et al {\ldots} practice that can
               help promote the values of openness, transparency,
               reproducibility , and more {\ldots} Although preregistration is a
               relatively new research practice in the social and applied {\ldots}},
	author = {Toth, Allison A and Banks, George C and Mellor, David and O'Boyle, Ernest H and Dickson, Ashleigh and Davis, Daniel J and DeHaven, Alex and Bochantin, Jaime and Borns, Jared},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {J. Bus. Psychol.},
	language = {en},
	month = aug,
	number = 4,
	pages = {553--571},
	publisher = {Springer Science and Business Media LLC},
	title = {Study preregistration: An evaluation of a method for transparent reporting},
	volume = 36,
	year = 2021}

@article{Troeger2019-pw,
	abstract = {{\ldots} In political science the EGAP registry holds 1128
               pre-registered research designs, as {\ldots} addition, recent research
               indicates that few studies that actually preregister follow
               through {\ldots} fraud more costly, instils better norms of
               replicability and reproducibility , pre - registration of
               research {\ldots}},
	author = {Troeger, Vera E},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Swiss Polit. Sci. Rev.},
	language = {en},
	month = sep,
	number = 3,
	pages = {281--287},
	publisher = {Wiley},
	title = {To {P} or not to P? The usefulness of Pvalues in quantitative political science research},
	volume = 25,
	year = 2019}

@article{Wicherts2012-co,
	abstract = {The authors argue that upon publication of a paper, the data
              should be made available through online archives or repositories.
              Reasons for not sharing data are discussed and contrasted with
              advantages of sharing, which include abiding by the scientific
              principle of openness, keeping the data for posterity, increasing
              one's impact, facilitation of secondary analyses and
              collaborations, prevention and correction of errors, and meeting
              funding agencies' increasingly stringent stipulations concerning
              the dissemination of data. Practicing what they preach, the
              authors include data as an online appendix to this editorial.
              These data are from a cohort of psychology freshmen who completed
              Raven's Advanced Progressive Matrices, tests of Numerical
              Ability, Number Series, Hidden Figures, Vocabulary, Verbal
              Analogies, and Logical Reasoning, two Big Five personality
              inventories, and scales for social desirability and impression
              management. Student's sex and grade point average (GPA) are also
              included. Data could be used to study predictive validity of
              cognitive ability tests, Extraversion, Neuroticism,
              Conscientiousness, Openness to Experience, Agreeableness, and the
              general factor of personality, as well as sex differences,
              differential prediction, and relations between personality and
              intelligence.},
	author = {Wicherts, Jelte M and Bakker, Marjan},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Intelligence},
	keywords = {Data sharing; Science policy; Data archiving; GPA;trust\_us},
	month = mar,
	number = 2,
	pages = {73--76},
	title = {Publish (your data) or (let the data) perish! Why not publish your data too?},
	volume = 40,
	year = 2012}

@article{Willinsky2005-hc,
	abstract = {This paper seeks to make the convergence of open source
               software, open access to research and scholarship, and open
               science apparent, as well as worth pursuing.},
	author = {Willinsky, John},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	language = {en},
	month = aug,
	publisher = {Valauskas, Edward J.},
	title = {The unacknowledged convergence of open source, open access, and open science},
	year = 2005}

@article{Wuffle2015-ab,
	abstract = {ABSTRACTThis essay consists of idiosynratic reflections on
               research methodology based on a long career.},
	author = {Wuffle, A},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	month = jan,
	number = 01,
	pages = {176--182},
	publisher = {Cambridge University Press (CUP)},
	title = {Uncle wuffle's reflections on political science methodology},
	volume = 48,
	year = 2015}

@article{Wuttke2019-iy,
	abstract = {Witnessing the ongoing ``credibility revolutions'' in other
               disciplines, political science should also engage in
               meta-scientific introspection. Theoretically, this commentary
               describes why scientists in academia's current incentive system
               work against their self-interest if they prioritize research
               credibility. Empirically, a comprehensive review of
               meta-scientific research with a focus on quantitative political
               science demonstrates that threats to the credibility of
               political science findings are systematic and real. Yet, the
               review also shows the discipline's recent progress toward more
               credible research. The commentary proposes specific
               institutional changes to better align individual researcher
               rationality with the collective good of verifiable, robust, and
               valid scientific results.},
	author = {Wuttke, Alexander},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Polit. Vierteljahresschr.},
	month = mar,
	number = 1,
	pages = {1--19},
	publisher = {Springer},
	title = {Why Too Many Political Science Findings Cannot Be Trusted and What We Can Do About It: A Review of {Meta-Scientific} Research and a Call for Academic Reform},
	volume = 60,
	year = 2019}

@article{Wuttke2019-yk,
	author = {Wuttke, Alexander},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {Polit. Vierteljahresschr.},
	language = {en},
	month = mar,
	number = 1,
	pages = {1--19},
	publisher = {Springer Science and Business Media LLC},
	title = {Why too many political science findings cannot be trusted and what we can do about it: A review of meta-scientific research and a call for academic reform},
	volume = 60,
	year = 2019}

@article{Yom2018-pn,
	abstract = {ABSTRACTAs a pillar of Data Access and Research Transparency
               (DA-RT), analytic transparency calls for radical honesty about
               how political scientists infer conclusions from their data.
               However, honesty about one's research practices often means
               discarding the linguistic template of deductive proceduralism
               that structures most writing, which in turn diminishes the
               prospects for successful publication. This dissonance reflects a
               unique dilemma: transparency initiatives reflect a vision of
               research drawn from the biomedical and natural sciences, and
               struggle with the messier, iterative, and open-ended nature of
               political science scholarship. Analytic transparency requires
               not only better individual practices, such as active citations,
               but also institutional strategies that reward radical honesty.
               Journals can provide authors with protected space to reveal
               research practices, further blind the review process, and
               experiment with special issues. More broadly, analytic openness
               can be mandated through procedural monitoring, such as real-time
               recording of research activities and keystroke logging for
               statistical programs.},
	author = {Yom, Sean},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	keywords = {trust\_us},
	month = apr,
	number = 02,
	pages = {416--421},
	publisher = {Cambridge University Press (CUP)},
	title = {Analytic transparency, radical honesty, and strategic incentives},
	volume = 51,
	year = 2018}

@article{Zigerell2017-um,
	abstract = {ABSTRACTPolitical science researchers have flexibility in how to
               analyze data, how to report data, and whether to report on data.
               A review of examples of reporting flexibility from the race and
               sex discrimination literature illustrates how research design
               choices can influence estimates and inferences. This reporting
               flexibility---coupled with the political imbalance among
               political scientists---creates the potential for political bias
               in reported political science estimates. These biases can be
               reduced or eliminated through preregistration and preacceptance,
               with researchers committing to a research design before
               completing data collection. Removing the potential for reporting
               flexibility can raise the credibility of political science
               research.},
	author = {Zigerell, L J},
	date-added = {2021-09-05 23:56:31 +1000},
	date-modified = {2021-09-05 23:56:31 +1000},
	journal = {PS Polit. Sci. Polit.},
	keywords = {trust\_us},
	language = {en},
	month = jan,
	number = 01,
	pages = {179--183},
	publisher = {Cambridge University Press (CUP)},
	title = {Reducing political bias in political science estimates},
	volume = 50,
	year = 2017}
